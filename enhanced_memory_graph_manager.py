"""
å¢å¼ºç‰ˆè®°å¿†å›¾è°±ç®¡ç†å™¨
é›†æˆé«˜çº§å®ä½“æå–ã€ä¼˜åŒ–å›¾è°±å¼•æ“å’Œå†²çªè§£å†³
"""
import asyncio
import time
from typing import Dict, List, Any, Optional, Tuple
from loguru import logger

from advanced_entity_extractor import AdvancedEntityExtractor, ExtractedEntity, ConflictInfo
from optimized_graph_engine import OptimizedGraphEngine, GraphNode, GraphEdge, SearchResult

class EnhancedMemoryGraphManager:
    """å¢å¼ºç‰ˆè®°å¿†å›¾è°±ç®¡ç†å™¨"""
    
    def __init__(self, openai_client, db_connection):
        self.openai_client = openai_client
        self.conn = db_connection
        
        # åˆå§‹åŒ–å­ç³»ç»Ÿ
        self.entity_extractor = AdvancedEntityExtractor(openai_client)
        self.graph_engine = OptimizedGraphEngine(db_connection)
        
        # é…ç½®å‚æ•°
        self.auto_conflict_resolution = True
        self.max_search_results = 20
        self.search_timeout = 5.0
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            "total_extractions": 0,
            "successful_extractions": 0,
            "total_searches": 0,
            "successful_searches": 0,
            "avg_extraction_time": 0.0,
            "avg_search_time": 0.0,
            "conflicts_detected": 0,
            "conflicts_resolved": 0
        }
        
        logger.info("ğŸš€ å¢å¼ºç‰ˆè®°å¿†å›¾è°±ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def add_memory_advanced(self, memory_id: str, content: str, 
                                memory_type: str, importance: float,
                                context: str = "") -> bool:
        """é«˜çº§è®°å¿†æ·»åŠ """
        try:
            start_time = time.time()
            self.performance_metrics["total_extractions"] += 1
            
            logger.debug(f"ğŸ§  é«˜çº§è®°å¿†æ·»åŠ : {content}")
            
            # 1. é«˜çº§å®ä½“æå–
            entities, conflicts = await self.entity_extractor.extract_entities_advanced(
                content, context
            )
            
            if conflicts:
                self.performance_metrics["conflicts_detected"] += len(conflicts)
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ° {len(conflicts)} ä¸ªå†²çª")
                
                # å¤„ç†å†²çª
                if self.auto_conflict_resolution:
                    for conflict in conflicts:
                        resolution = await self.entity_extractor.resolve_conflict(conflict)
                        logger.info(f"ğŸ”§ å†²çªè§£å†³: {conflict.entity} -> {resolution}")
                        self.performance_metrics["conflicts_resolved"] += 1
            
            # 2. åˆ›å»ºå›¾èŠ‚ç‚¹
            node_properties = {
                "memory_type": memory_type,
                "context": context,
                "entities": [entity.text for entity in entities],
                "entity_types": [entity.entity_type.value for entity in entities]
            }
            
            graph_node = GraphNode(
                id=memory_id,
                content=content,
                node_type=memory_type,
                importance=importance,
                properties=node_properties,
                created_time=time.time(),
                last_accessed=time.time(),
                access_count=0
            )
            
            # 3. æ·»åŠ åˆ°å›¾å¼•æ“
            entity_texts = [entity.normalized_form for entity in entities]
            success = await self.graph_engine.add_node(graph_node, entity_texts)
            
            if success:
                # 4. æ„å»ºå…³è”è¾¹
                await self._build_advanced_relations(graph_node, entities)
                
                self.performance_metrics["successful_extractions"] += 1
                
                extraction_time = time.time() - start_time
                self._update_extraction_time(extraction_time)
                
                logger.info(f"âœ… é«˜çº§è®°å¿†æ·»åŠ æˆåŠŸ: {len(entities)} ä¸ªå®ä½“, è€—æ—¶ {extraction_time:.3f}s")
                return True
            else:
                logger.error(f"âŒ å›¾èŠ‚ç‚¹æ·»åŠ å¤±è´¥: {memory_id}")
                return False
            
        except Exception as e:
            logger.error(f"é«˜çº§è®°å¿†æ·»åŠ å¤±è´¥: {e}")
            return False
    
    async def _build_advanced_relations(self, node: GraphNode, entities: List[ExtractedEntity]):
        """æ„å»ºé«˜çº§å…³è”å…³ç³»"""
        try:
            # 1. å®ä½“å…³è”
            for entity in entities:
                # æŸ¥æ‰¾å…·æœ‰ç›¸åŒå®ä½“çš„å…¶ä»–èŠ‚ç‚¹
                similar_nodes = await self.graph_engine.search_nodes_by_entities(
                    [entity.normalized_form] + entity.aliases, max_results=10
                )
                
                for similar_node in similar_nodes:
                    if similar_node.id != node.id:
                        # åˆ›å»ºå®ä½“å…³è”è¾¹
                        edge = GraphEdge(
                            id=f"{node.id}_{similar_node.id}_entity_{entity.normalized_form}",
                            source_id=node.id,
                            target_id=similar_node.id,
                            edge_type=f"entity_{entity.entity_type.value}",
                            weight=1.0,
                            confidence=entity.confidence,
                            properties={
                                "entity": entity.normalized_form,
                                "entity_type": entity.entity_type.value,
                                "relation_reason": f"å…±åŒå®ä½“: {entity.normalized_form}"
                            },
                            created_time=time.time()
                        )
                        
                        await self.graph_engine.add_edge(edge)
            
            # 2. è¯­ä¹‰å…³è”
            await self._build_semantic_relations(node)
            
            # 3. é‡è¦æ€§å…³è”
            await self._build_importance_relations(node)
            
        except Exception as e:
            logger.error(f"æ„å»ºé«˜çº§å…³è”å¤±è´¥: {e}")
    
    async def _build_semantic_relations(self, node: GraphNode):
        """æ„å»ºè¯­ä¹‰å…³è”"""
        try:
            # æŸ¥æ‰¾é«˜é‡è¦æ€§èŠ‚ç‚¹è¿›è¡Œè¯­ä¹‰å…³è”
            high_importance_nodes = await self.graph_engine.search_by_importance(
                min_importance=0.8, max_results=10
            )
            
            for other_node in high_importance_nodes:
                if other_node.id != node.id:
                    # ä½¿ç”¨AIåˆ†æè¯­ä¹‰å…³è”
                    relation = await self._analyze_semantic_similarity(node, other_node)
                    
                    if relation and relation.confidence > 0.7:
                        edge = GraphEdge(
                            id=f"{node.id}_{other_node.id}_semantic",
                            source_id=node.id,
                            target_id=other_node.id,
                            edge_type="semantic",
                            weight=relation.confidence,
                            confidence=relation.confidence,
                            properties={
                                "relation_type": "semantic_similarity",
                                "description": relation.description
                            },
                            created_time=time.time()
                        )
                        
                        await self.graph_engine.add_edge(edge)
            
        except Exception as e:
            logger.error(f"æ„å»ºè¯­ä¹‰å…³è”å¤±è´¥: {e}")
    
    async def _analyze_semantic_similarity(self, node1: GraphNode, node2: GraphNode) -> Optional[Any]:
        """åˆ†æè¯­ä¹‰ç›¸ä¼¼æ€§"""
        try:
            prompt = f"""
åˆ†æä»¥ä¸‹ä¸¤ä¸ªè®°å¿†ç‰‡æ®µçš„è¯­ä¹‰å…³è”ï¼š

è®°å¿†1: {node1.content}
è®°å¿†2: {node2.content}

è¯·åˆ¤æ–­å®ƒä»¬æ˜¯å¦æœ‰è¯­ä¹‰å…³è”ï¼Œå¦‚æœæœ‰ï¼Œç»™å‡ºå…³è”ç±»å‹å’Œç½®ä¿¡åº¦ã€‚

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
    "has_relation": true/false,
    "confidence": 0.0-1.0,
    "relation_type": "family/work/location/preference/other",
    "description": "å…³è”æè¿°"
}}
"""

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=150,
                temperature=0.1
            )
            
            if response.choices:
                import json
                try:
                    result = json.loads(response.choices[0].message.content)
                    if result.get("has_relation", False):
                        from dataclasses import dataclass
                        
                        @dataclass
                        class SemanticRelation:
                            confidence: float
                            description: str
                        
                        return SemanticRelation(
                            confidence=result.get("confidence", 0.5),
                            description=result.get("description", "è¯­ä¹‰å…³è”")
                        )
                except json.JSONDecodeError:
                    logger.debug("è¯­ä¹‰åˆ†æJSONè§£æå¤±è´¥")
            
            return None
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰ç›¸ä¼¼æ€§åˆ†æå¤±è´¥: {e}")
            return None
    
    async def _build_importance_relations(self, node: GraphNode):
        """æ„å»ºé‡è¦æ€§å…³è”"""
        try:
            # å¦‚æœæ˜¯é«˜é‡è¦æ€§èŠ‚ç‚¹ï¼Œä¸å…¶ä»–é«˜é‡è¦æ€§èŠ‚ç‚¹å»ºç«‹å…³è”
            if node.importance >= 0.8:
                similar_importance_nodes = await self.graph_engine.search_by_importance(
                    min_importance=node.importance - 0.1, max_results=5
                )
                
                for other_node in similar_importance_nodes:
                    if (other_node.id != node.id and 
                        other_node.node_type == node.node_type):
                        
                        edge = GraphEdge(
                            id=f"{node.id}_{other_node.id}_importance",
                            source_id=node.id,
                            target_id=other_node.id,
                            edge_type="importance_cluster",
                            weight=min(node.importance, other_node.importance),
                            confidence=0.8,
                            properties={
                                "relation_type": "similar_importance",
                                "importance_level": "high"
                            },
                            created_time=time.time()
                        )
                        
                        await self.graph_engine.add_edge(edge)
            
        except Exception as e:
            logger.error(f"æ„å»ºé‡è¦æ€§å…³è”å¤±è´¥: {e}")
    
    async def enhanced_search_and_respond(self, question: str, context: str = "") -> Tuple[str, List[str]]:
        """å¢å¼ºæœç´¢å¹¶å›ç­”"""
        try:
            start_time = time.time()
            self.performance_metrics["total_searches"] += 1
            
            logger.info(f"ğŸ” å¢å¼ºæœç´¢: {question}")
            
            # 1. ä»é—®é¢˜ä¸­æå–å®ä½“
            question_entities, _ = await self.entity_extractor.extract_entities_advanced(question, context)
            entity_texts = [entity.normalized_form for entity in question_entities]
            
            logger.debug(f"é—®é¢˜å®ä½“: {entity_texts}")
            
            # 2. å¤šç­–ç•¥æœç´¢
            search_strategies = ["hybrid", "entity_first", "importance_first"]
            best_result = None
            best_confidence = 0.0
            
            for strategy in search_strategies:
                try:
                    result = await asyncio.wait_for(
                        self.graph_engine.advanced_search(question, entity_texts, strategy),
                        timeout=self.search_timeout
                    )
                    
                    if result.confidence > best_confidence:
                        best_result = result
                        best_confidence = result.confidence
                        
                except asyncio.TimeoutError:
                    logger.warning(f"æœç´¢ç­–ç•¥ {strategy} è¶…æ—¶")
                    continue
            
            if not best_result or not best_result.nodes:
                logger.info("ğŸ“­ æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
                return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", []
            
            # 3. æ›´æ–°è®¿é—®ç»Ÿè®¡
            for node in best_result.nodes:
                await self.graph_engine.update_node_access(node.id)
            
            # 4. ç”Ÿæˆæ™ºèƒ½å›ç­”
            response = await self._generate_enhanced_response(
                question, best_result, context
            )
            
            used_memory_ids = [node.id for node in best_result.nodes]
            
            search_time = time.time() - start_time
            self._update_search_time(search_time)
            self.performance_metrics["successful_searches"] += 1
            
            logger.info(f"âœ… å¢å¼ºæœç´¢å®Œæˆ: {len(used_memory_ids)} æ¡è®°å¿†, è€—æ—¶ {search_time:.3f}s")
            return response, used_memory_ids
            
        except Exception as e:
            logger.error(f"å¢å¼ºæœç´¢å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæœç´¢è¿‡ç¨‹ä¸­å‡ºç°äº†é”™è¯¯ã€‚", []
    
    async def _generate_enhanced_response(self, question: str, search_result: SearchResult, context: str) -> str:
        """ç”Ÿæˆå¢å¼ºå›ç­”"""
        try:
            # æ„å»ºä¸°å¯Œçš„ä¸Šä¸‹æ–‡
            memory_context = []
            reasoning_context = []
            
            # æŒ‰é‡è¦æ€§æ’åºèŠ‚ç‚¹
            sorted_nodes = sorted(search_result.nodes, key=lambda x: x.importance, reverse=True)
            
            for node in sorted_nodes[:10]:  # æœ€å¤š10ä¸ªèŠ‚ç‚¹
                memory_context.append(f"- {node.content} (é‡è¦æ€§: {node.importance:.2f})")
            
            # æ¨ç†è·¯å¾„
            for step in search_result.reasoning_path:
                reasoning_context.append(f"â†’ {step}")
            
            # å…³è”ä¿¡æ¯
            if search_result.edges:
                relation_info = []
                for edge in search_result.edges[:5]:  # æœ€å¤š5ä¸ªå…³è”
                    relation_info.append(f"  {edge.edge_type}: {edge.properties.get('description', 'å…³è”')}")
                reasoning_context.extend(relation_info)
            
            memory_text = "\n".join(memory_context)
            reasoning_text = "\n".join(reasoning_context)
            
            prompt = f"""åŸºäºä»¥ä¸‹è®°å¿†å›¾è°±ä¿¡æ¯å’Œæ¨ç†è·¯å¾„ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚

é—®é¢˜: {question}
ä¸Šä¸‹æ–‡: {context}

è®°å¿†ä¿¡æ¯:
{memory_text}

æ¨ç†è·¯å¾„:
{reasoning_text}

è¦æ±‚:
1. åŸºäºè®°å¿†ä¿¡æ¯è¿›è¡Œæ¨ç†å›ç­”
2. åˆ©ç”¨æ¨ç†è·¯å¾„ä¸­çš„å…³è”ä¿¡æ¯
3. å¦‚æœèƒ½æ¨ç†å‡ºç­”æ¡ˆï¼Œè¯´æ˜æ¨ç†è¿‡ç¨‹
4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®æŒ‡å‡ºç¼ºå°‘ä»€ä¹ˆ
5. å›ç­”è¦è‡ªç„¶ã€å£è¯­åŒ–
6. 1-2å¥è¯ç®€æ´å›ç­”

è¯·ç›´æ¥ç»™å‡ºå›ç­”ï¼š"""

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.7
            )
            
            if response.choices:
                return response.choices[0].message.content.strip()
            
            return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚"
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¢å¼ºå›ç­”å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œå›ç­”ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°äº†é”™è¯¯ã€‚"
    
    async def batch_add_memories(self, memories: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰¹é‡æ·»åŠ è®°å¿†"""
        try:
            logger.info(f"ğŸ“¦ æ‰¹é‡æ·»åŠ  {len(memories)} æ¡è®°å¿†")
            
            start_time = time.time()
            successes = 0
            failures = 0
            errors = []
            
            # å¹¶å‘å¤„ç†ï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰
            semaphore = asyncio.Semaphore(5)  # æœ€å¤š5ä¸ªå¹¶å‘
            
            async def process_memory(memory_data):
                async with semaphore:
                    try:
                        success = await self.add_memory_advanced(
                            memory_data["id"],
                            memory_data["content"],
                            memory_data.get("memory_type", "general"),
                            memory_data.get("importance", 0.5),
                            memory_data.get("context", "")
                        )
                        return success, None
                    except Exception as e:
                        return False, str(e)
            
            # æ‰§è¡Œæ‰¹é‡å¤„ç†
            tasks = [process_memory(memory) for memory in memories]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, tuple):
                    success, error = result
                    if success:
                        successes += 1
                    else:
                        failures += 1
                        if error:
                            errors.append(f"è®°å¿†{i}: {error}")
                else:
                    failures += 1
                    errors.append(f"è®°å¿†{i}: {str(result)}")
            
            total_time = time.time() - start_time
            
            logger.info(f"ğŸ“¦ æ‰¹é‡æ·»åŠ å®Œæˆ: {successes}æˆåŠŸ / {failures}å¤±è´¥, è€—æ—¶ {total_time:.1f}s")
            
            return {
                "total": len(memories),
                "successes": successes,
                "failures": failures,
                "success_rate": successes / len(memories) * 100,
                "total_time": total_time,
                "throughput": len(memories) / total_time,
                "errors": errors[:10]  # æœ€å¤šæ˜¾ç¤º10ä¸ªé”™è¯¯
            }
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ·»åŠ è®°å¿†å¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def analyze_memory_network(self) -> Dict[str, Any]:
        """åˆ†æè®°å¿†ç½‘ç»œ"""
        try:
            logger.info("ğŸ“Š åˆ†æè®°å¿†ç½‘ç»œç»“æ„")
            
            # è·å–å›¾å¼•æ“æ€§èƒ½ç»Ÿè®¡
            graph_stats = self.graph_engine.get_performance_stats()
            
            # åˆ†æç½‘ç»œå¯†åº¦
            if graph_stats["total_nodes"] > 0 and graph_stats["total_edges"] > 0:
                # ç½‘ç»œå¯†åº¦ = å®é™…è¾¹æ•° / æœ€å¤§å¯èƒ½è¾¹æ•°
                max_edges = graph_stats["total_nodes"] * (graph_stats["total_nodes"] - 1) / 2
                network_density = graph_stats["total_edges"] / max_edges if max_edges > 0 else 0
            else:
                network_density = 0
            
            # åˆ†æå®ä½“åˆ†å¸ƒ
            entity_stats = await self._analyze_entity_distribution()
            
            # åˆ†æé‡è¦æ€§åˆ†å¸ƒ
            importance_stats = await self._analyze_importance_distribution()
            
            # åˆ†æè¿æ¥æ€§
            connectivity_stats = await self._analyze_connectivity()
            
            analysis = {
                "network_overview": {
                    "total_nodes": graph_stats["total_nodes"],
                    "total_edges": graph_stats["total_edges"],
                    "network_density": network_density,
                    "avg_search_time": graph_stats["avg_search_time"],
                    "cache_hit_rate": graph_stats.get("cache_hit_rate", 0)
                },
                "entity_distribution": entity_stats,
                "importance_distribution": importance_stats,
                "connectivity_analysis": connectivity_stats,
                "performance_metrics": self.performance_metrics,
                "recommendations": self._generate_network_recommendations(graph_stats, network_density)
            }
            
            logger.info(f"ğŸ“Š ç½‘ç»œåˆ†æå®Œæˆ: {graph_stats['total_nodes']} èŠ‚ç‚¹, å¯†åº¦ {network_density:.3f}")
            
            return analysis
            
        except Exception as e:
            logger.error(f"è®°å¿†ç½‘ç»œåˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def _analyze_entity_distribution(self) -> Dict[str, Any]:
        """åˆ†æå®ä½“åˆ†å¸ƒ"""
        try:
            cursor = self.conn.execute("""
                SELECT entity, COUNT(*) as count 
                FROM entity_node_index 
                GROUP BY entity 
                ORDER BY count DESC 
                LIMIT 20
            """)
            
            entity_counts = dict(cursor.fetchall())
            
            # ç»Ÿè®¡å®ä½“ç±»å‹
            cursor = self.conn.execute("""
                SELECT 
                    json_extract(properties, '$.entity_types') as types,
                    COUNT(*) as count
                FROM graph_nodes 
                WHERE json_extract(properties, '$.entity_types') IS NOT NULL
                GROUP BY types
            """)
            
            type_distribution = {}
            for types_json, count in cursor.fetchall():
                if types_json:
                    import json
                    try:
                        types = json.loads(types_json)
                        for entity_type in types:
                            type_distribution[entity_type] = type_distribution.get(entity_type, 0) + count
                    except:
                        pass
            
            return {
                "top_entities": entity_counts,
                "entity_type_distribution": type_distribution,
                "total_unique_entities": len(entity_counts),
                "avg_entity_frequency": sum(entity_counts.values()) / len(entity_counts) if entity_counts else 0
            }
            
        except Exception as e:
            logger.error(f"å®ä½“åˆ†å¸ƒåˆ†æå¤±è´¥: {e}")
            return {}
    
    async def _analyze_importance_distribution(self) -> Dict[str, Any]:
        """åˆ†æé‡è¦æ€§åˆ†å¸ƒ"""
        try:
            cursor = self.conn.execute("""
                SELECT 
                    CASE 
                        WHEN importance >= 0.9 THEN 'very_high'
                        WHEN importance >= 0.7 THEN 'high'
                        WHEN importance >= 0.5 THEN 'medium'
                        WHEN importance >= 0.3 THEN 'low'
                        ELSE 'very_low'
                    END as importance_level,
                    COUNT(*) as count,
                    AVG(importance) as avg_importance
                FROM graph_nodes
                GROUP BY importance_level
                ORDER BY avg_importance DESC
            """)
            
            distribution = {}
            for level, count, avg_imp in cursor.fetchall():
                distribution[level] = {
                    "count": count,
                    "avg_importance": round(avg_imp, 3)
                }
            
            # æ€»ä½“ç»Ÿè®¡
            cursor = self.conn.execute("SELECT AVG(importance), MIN(importance), MAX(importance) FROM graph_nodes")
            overall_avg, min_imp, max_imp = cursor.fetchone()
            
            return {
                "distribution": distribution,
                "overall_avg": round(overall_avg or 0, 3),
                "min_importance": round(min_imp or 0, 3),
                "max_importance": round(max_imp or 0, 3)
            }
            
        except Exception as e:
            logger.error(f"é‡è¦æ€§åˆ†å¸ƒåˆ†æå¤±è´¥: {e}")
            return {}
    
    async def _analyze_connectivity(self) -> Dict[str, Any]:
        """åˆ†æè¿æ¥æ€§"""
        try:
            # è®¡ç®—å¹³å‡åº¦æ•°
            cursor = self.conn.execute("""
                SELECT node_id, COUNT(*) as degree
                FROM (
                    SELECT source_id as node_id FROM graph_edges
                    UNION ALL
                    SELECT target_id as node_id FROM graph_edges
                ) 
                GROUP BY node_id
            """)
            
            degrees = [row[1] for row in cursor.fetchall()]
            
            if degrees:
                avg_degree = sum(degrees) / len(degrees)
                max_degree = max(degrees)
                min_degree = min(degrees)
            else:
                avg_degree = max_degree = min_degree = 0
            
            # å­¤ç«‹èŠ‚ç‚¹æ•°é‡
            cursor = self.conn.execute("""
                SELECT COUNT(*) FROM graph_nodes 
                WHERE id NOT IN (
                    SELECT DISTINCT source_id FROM graph_edges
                    UNION
                    SELECT DISTINCT target_id FROM graph_edges
                )
            """)
            isolated_nodes = cursor.fetchone()[0]
            
            # è¾¹ç±»å‹åˆ†å¸ƒ
            cursor = self.conn.execute("""
                SELECT edge_type, COUNT(*) as count 
                FROM graph_edges 
                GROUP BY edge_type 
                ORDER BY count DESC
            """)
            edge_type_distribution = dict(cursor.fetchall())
            
            return {
                "avg_degree": round(avg_degree, 2),
                "max_degree": max_degree,
                "min_degree": min_degree,
                "isolated_nodes": isolated_nodes,
                "edge_type_distribution": edge_type_distribution,
                "connectivity_ratio": (len(degrees) / self.graph_engine.performance_stats["total_nodes"]) if self.graph_engine.performance_stats["total_nodes"] > 0 else 0
            }
            
        except Exception as e:
            logger.error(f"è¿æ¥æ€§åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _generate_network_recommendations(self, graph_stats: Dict, network_density: float) -> List[str]:
        """ç”Ÿæˆç½‘ç»œä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # å¯†åº¦å»ºè®®
        if network_density < 0.1:
            recommendations.append("ç½‘ç»œå¯†åº¦è¾ƒä½ï¼Œå»ºè®®å¢å¼ºå®ä½“å…³è”ç®—æ³•")
        elif network_density > 0.8:
            recommendations.append("ç½‘ç»œå¯†åº¦è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨å†—ä½™å…³è”ï¼Œå»ºè®®ä¼˜åŒ–å…³è”è´¨é‡")
        
        # æ€§èƒ½å»ºè®®
        if graph_stats.get("avg_search_time", 0) > 1.0:
            recommendations.append("æœç´¢æ€§èƒ½è¾ƒæ…¢ï¼Œå»ºè®®ä¼˜åŒ–ç´¢å¼•å’Œç¼“å­˜ç­–ç•¥")
        
        # ç¼“å­˜å»ºè®®
        if graph_stats.get("cache_hit_rate", 0) < 0.5:
            recommendations.append("ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œå»ºè®®è°ƒæ•´ç¼“å­˜ç­–ç•¥å’Œå¤§å°")
        
        # è§„æ¨¡å»ºè®®
        if graph_stats["total_nodes"] > 10000:
            recommendations.append("èŠ‚ç‚¹æ•°é‡è¾ƒå¤§ï¼Œå»ºè®®å¯ç”¨åˆ†å¸ƒå¼å¤„ç†å’Œæ•°æ®åˆ†ç‰‡")
        
        if not recommendations:
            recommendations.append("ç½‘ç»œç»“æ„è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰é…ç½®")
        
        return recommendations
    
    def _update_extraction_time(self, time_taken: float):
        """æ›´æ–°æå–æ—¶é—´ç»Ÿè®¡"""
        total_time = (self.performance_metrics["avg_extraction_time"] * 
                     (self.performance_metrics["successful_extractions"] - 1))
        self.performance_metrics["avg_extraction_time"] = (
            (total_time + time_taken) / self.performance_metrics["successful_extractions"]
        )
    
    def _update_search_time(self, time_taken: float):
        """æ›´æ–°æœç´¢æ—¶é—´ç»Ÿè®¡"""
        total_time = (self.performance_metrics["avg_search_time"] * 
                     (self.performance_metrics["successful_searches"] - 1))
        self.performance_metrics["avg_search_time"] = (
            (total_time + time_taken) / self.performance_metrics["successful_searches"]
        )
    
    async def optimize_network(self) -> Dict[str, Any]:
        """ä¼˜åŒ–ç½‘ç»œç»“æ„"""
        try:
            logger.info("ğŸ”§ å¼€å§‹ç½‘ç»œä¼˜åŒ–...")
            
            start_time = time.time()
            optimizations = []
            
            # 1. æ¸…ç†ä½ä»·å€¼èŠ‚ç‚¹
            cleaned_count = await self.graph_engine.cleanup_low_value_nodes(
                min_importance=0.2, min_access_count=1, days_threshold=30
            )
            
            if cleaned_count > 0:
                optimizations.append(f"æ¸…ç†äº† {cleaned_count} ä¸ªä½ä»·å€¼èŠ‚ç‚¹")
            
            # 2. é‡å»ºç´¢å¼•
            self.graph_engine._build_indexes()
            optimizations.append("é‡å»ºäº†å†…å­˜ç´¢å¼•")
            
            # 3. æ¸…ç†ç¼“å­˜
            self.graph_engine.node_cache.clear()
            self.graph_engine.edge_cache.clear()
            self.graph_engine.search_cache.clear()
            optimizations.append("æ¸…ç†äº†æ‰€æœ‰ç¼“å­˜")
            
            optimization_time = time.time() - start_time
            
            logger.info(f"ğŸ”§ ç½‘ç»œä¼˜åŒ–å®Œæˆ: è€—æ—¶ {optimization_time:.1f}s")
            
            return {
                "optimization_time": optimization_time,
                "optimizations_applied": optimizations,
                "nodes_cleaned": cleaned_count
            }
            
        except Exception as e:
            logger.error(f"ç½‘ç»œä¼˜åŒ–å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        try:
            graph_stats = self.graph_engine.get_performance_stats()
            
            return {
                "system_health": "healthy" if graph_stats.get("avg_search_time", 0) < 2.0 else "degraded",
                "performance_metrics": self.performance_metrics,
                "graph_statistics": graph_stats,
                "extractor_status": "active",
                "auto_conflict_resolution": self.auto_conflict_resolution,
                "search_timeout": self.search_timeout,
                "max_search_results": self.max_search_results
            }
            
        except Exception as e:
            logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def close(self):
        """å…³é—­ç®¡ç†å™¨"""
        try:
            if hasattr(self.graph_engine, 'close'):
                self.graph_engine.close()
            
            logger.info("ğŸš€ å¢å¼ºç‰ˆè®°å¿†å›¾è°±ç®¡ç†å™¨å·²å…³é—­")
            
        except Exception as e:
            logger.error(f"å…³é—­ç®¡ç†å™¨å¤±è´¥: {e}")