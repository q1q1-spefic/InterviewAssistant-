"""
ä¼˜åŒ–å›¾è°±å¼•æ“
æ”¯æŒå¤§è§„æ¨¡æ•°æ®ã€å¿«é€Ÿæœç´¢ã€æ™ºèƒ½ç¼“å­˜å’Œåˆ†å¸ƒå¼å¤„ç†
"""
import asyncio
import json
import time
import sqlite3
import hashlib
from typing import Dict, List, Tuple, Optional, Any, Set
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor
import pickle
import lru
from loguru import logger

@dataclass
class GraphNode:
    """å›¾èŠ‚ç‚¹"""
    id: str
    content: str
    node_type: str
    importance: float
    properties: Dict[str, Any]
    created_time: float
    last_accessed: float
    access_count: int

@dataclass
class GraphEdge:
    """å›¾è¾¹"""
    id: str
    source_id: str
    target_id: str
    edge_type: str
    weight: float
    confidence: float
    properties: Dict[str, Any]
    created_time: float

@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    nodes: List[GraphNode]
    edges: List[GraphEdge]
    reasoning_path: List[str]
    confidence: float
    search_time: float

class OptimizedGraphEngine:
    """ä¼˜åŒ–å›¾è°±å¼•æ“"""
    
    def __init__(self, db_connection, cache_size: int = 10000):
        self.conn = db_connection
        self.cache_size = cache_size
        
        # é«˜æ€§èƒ½ç¼“å­˜
        self.node_cache = lru.LRU(cache_size)
        self.edge_cache = lru.LRU(cache_size)
        self.search_cache = lru.LRU(cache_size // 10)
        
        # ç´¢å¼•ç»“æ„
        self.entity_index = defaultdict(set)  # entity -> node_ids
        self.type_index = defaultdict(set)    # type -> node_ids
        self.importance_index = []            # [(importance, node_id)]
        
        # æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            "total_searches": 0,
            "cache_hits": 0,
            "avg_search_time": 0.0,
            "total_nodes": 0,
            "total_edges": 0
        }
        
        # çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        self._init_optimized_storage()
        self._build_indexes()
        
        logger.info("âš¡ ä¼˜åŒ–å›¾è°±å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _init_optimized_storage(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å­˜å‚¨"""
        try:
            # ä¼˜åŒ–çš„èŠ‚ç‚¹è¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS graph_nodes (
                    id TEXT PRIMARY KEY,
                    content TEXT NOT NULL,
                    node_type TEXT NOT NULL,
                    importance REAL NOT NULL,
                    properties TEXT,
                    created_time REAL NOT NULL,
                    last_accessed REAL DEFAULT 0,
                    access_count INTEGER DEFAULT 0
                )
            """)
            
            # ä¼˜åŒ–çš„è¾¹è¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS graph_edges (
                    id TEXT PRIMARY KEY,
                    source_id TEXT NOT NULL,
                    target_id TEXT NOT NULL,
                    edge_type TEXT NOT NULL,
                    weight REAL DEFAULT 1.0,
                    confidence REAL NOT NULL,
                    properties TEXT,
                    created_time REAL NOT NULL,
                    FOREIGN KEY (source_id) REFERENCES graph_nodes (id),
                    FOREIGN KEY (target_id) REFERENCES graph_nodes (id)
                )
            """)
            
            # é«˜æ€§èƒ½ç´¢å¼•è¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS entity_node_index (
                    entity TEXT NOT NULL,
                    node_id TEXT NOT NULL,
                    confidence REAL DEFAULT 1.0,
                    PRIMARY KEY (entity, node_id)
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            try:
                self.conn.execute("CREATE INDEX IF NOT EXISTS idx_nodes_type_importance ON graph_nodes (node_type, importance DESC)")
                self.conn.execute("CREATE INDEX IF NOT EXISTS idx_nodes_importance ON graph_nodes (importance DESC)")
                self.conn.execute("CREATE INDEX IF NOT EXISTS idx_edges_source ON graph_edges (source_id)")
                self.conn.execute("CREATE INDEX IF NOT EXISTS idx_edges_target ON graph_edges (target_id)")
                self.conn.execute("CREATE INDEX IF NOT EXISTS idx_edges_type ON graph_edges (edge_type)")
                self.conn.execute("CREATE INDEX IF NOT EXISTS idx_entity_index ON entity_node_index (entity)")
            except:
                pass  # ç´¢å¼•å¯èƒ½å·²å­˜åœ¨
            
            self.conn.commit()
            logger.debug("âœ… ä¼˜åŒ–å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _build_indexes(self):
        """æ„å»ºå†…å­˜ç´¢å¼•"""
        try:
            start_time = time.time()
            
            # æ„å»ºå®ä½“ç´¢å¼•
            cursor = self.conn.execute("SELECT entity, node_id FROM entity_node_index")
            for entity, node_id in cursor.fetchall():
                self.entity_index[entity].add(node_id)
            
            # æ„å»ºç±»å‹ç´¢å¼•
            cursor = self.conn.execute("SELECT node_type, id FROM graph_nodes")
            for node_type, node_id in cursor.fetchall():
                self.type_index[node_type].add(node_id)
            
            # æ„å»ºé‡è¦æ€§ç´¢å¼•
            cursor = self.conn.execute("SELECT importance, id FROM graph_nodes ORDER BY importance DESC")
            self.importance_index = list(cursor.fetchall())
            
            # æ›´æ–°ç»Ÿè®¡
            cursor = self.conn.execute("SELECT COUNT(*) FROM graph_nodes")
            self.performance_stats["total_nodes"] = cursor.fetchone()[0]
            
            cursor = self.conn.execute("SELECT COUNT(*) FROM graph_edges")
            self.performance_stats["total_edges"] = cursor.fetchone()[0]
            
            build_time = time.time() - start_time
            logger.info(f"ğŸ“Š ç´¢å¼•æ„å»ºå®Œæˆ: {self.performance_stats['total_nodes']} èŠ‚ç‚¹, "
                       f"{self.performance_stats['total_edges']} è¾¹, è€—æ—¶ {build_time:.3f}s")
            
        except Exception as e:
            logger.error(f"ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
    
    async def add_node(self, node: GraphNode, entities: List[str] = None) -> bool:
        """æ·»åŠ èŠ‚ç‚¹"""
        try:
            # å­˜å‚¨èŠ‚ç‚¹
            self.conn.execute("""
                INSERT OR REPLACE INTO graph_nodes 
                (id, content, node_type, importance, properties, created_time, last_accessed, access_count)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                node.id, node.content, node.node_type, node.importance,
                json.dumps(node.properties), node.created_time, 
                node.last_accessed, node.access_count
            ))
            
            # æ›´æ–°å®ä½“ç´¢å¼•
            if entities:
                for entity in entities:
                    self.conn.execute("""
                        INSERT OR REPLACE INTO entity_node_index (entity, node_id, confidence)
                        VALUES (?, ?, ?)
                    """, (entity, node.id, 1.0))
                    
                    # æ›´æ–°å†…å­˜ç´¢å¼•
                    self.entity_index[entity].add(node.id)
            
            # æ›´æ–°ç±»å‹ç´¢å¼•
            self.type_index[node.node_type].add(node.id)
            
            # æ›´æ–°é‡è¦æ€§ç´¢å¼•
            self.importance_index.append((node.importance, node.id))
            self.importance_index.sort(reverse=True)
            
            # ç¼“å­˜èŠ‚ç‚¹
            self.node_cache[node.id] = node
            
            self.conn.commit()
            self.performance_stats["total_nodes"] += 1
            
            return True
            
        except Exception as e:
            logger.error(f"æ·»åŠ èŠ‚ç‚¹å¤±è´¥: {e}")
            return False
    
    async def add_edge(self, edge: GraphEdge) -> bool:
        """æ·»åŠ è¾¹"""
        try:
            self.conn.execute("""
                INSERT OR REPLACE INTO graph_edges 
                (id, source_id, target_id, edge_type, weight, confidence, properties, created_time)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                edge.id, edge.source_id, edge.target_id, edge.edge_type,
                edge.weight, edge.confidence, json.dumps(edge.properties), edge.created_time
            ))
            
            # ç¼“å­˜è¾¹
            edge_key = f"{edge.source_id}:{edge.target_id}"
            if edge_key not in self.edge_cache:
                self.edge_cache[edge_key] = []
            self.edge_cache[edge_key].append(edge)
            
            self.conn.commit()
            self.performance_stats["total_edges"] += 1
            
            return True
            
        except Exception as e:
            logger.error(f"æ·»åŠ è¾¹å¤±è´¥: {e}")
            return False
    
    async def search_nodes_by_entities(self, entities: List[str], max_results: int = 50) -> List[GraphNode]:
        """é€šè¿‡å®ä½“æœç´¢èŠ‚ç‚¹"""
        try:
            start_time = time.time()
            
            # ä½¿ç”¨å†…å­˜ç´¢å¼•å¿«é€ŸæŸ¥æ‰¾
            candidate_node_ids = set()
            for entity in entities:
                if entity in self.entity_index:
                    candidate_node_ids.update(self.entity_index[entity])
            
            if not candidate_node_ids:
                return []
            
            # é™åˆ¶ç»“æœæ•°é‡
            candidate_node_ids = list(candidate_node_ids)[:max_results]
            
            # æ‰¹é‡è·å–èŠ‚ç‚¹
            nodes = await self._get_nodes_batch(candidate_node_ids)
            
            # æŒ‰é‡è¦æ€§æ’åº
            nodes.sort(key=lambda x: x.importance, reverse=True)
            
            search_time = time.time() - start_time
            self._update_performance_stats(search_time, len(nodes))
            
            return nodes[:max_results]
            
        except Exception as e:
            logger.error(f"å®ä½“æœç´¢å¤±è´¥: {e}")
            return []
    
    async def search_by_importance(self, min_importance: float = 0.7, max_results: int = 20) -> List[GraphNode]:
        """æŒ‰é‡è¦æ€§æœç´¢"""
        try:
            # ä½¿ç”¨é‡è¦æ€§ç´¢å¼•
            candidate_ids = [node_id for importance, node_id in self.importance_index 
                           if importance >= min_importance][:max_results]
            
            nodes = await self._get_nodes_batch(candidate_ids)
            return nodes
            
        except Exception as e:
            logger.error(f"é‡è¦æ€§æœç´¢å¤±è´¥: {e}")
            return []
    
    async def find_path(self, source_id: str, target_id: str, max_depth: int = 3) -> Optional[List[str]]:
        """æŸ¥æ‰¾ä¸¤ä¸ªèŠ‚ç‚¹é—´çš„è·¯å¾„"""
        try:
            # ä½¿ç”¨BFSæŸ¥æ‰¾æœ€çŸ­è·¯å¾„
            queue = deque([(source_id, [source_id])])
            visited = {source_id}
            
            while queue:
                current_id, path = queue.popleft()
                
                if len(path) > max_depth:
                    continue
                
                if current_id == target_id:
                    return path
                
                # è·å–é‚»å±…èŠ‚ç‚¹
                neighbors = await self._get_neighbors(current_id)
                
                for neighbor_id in neighbors:
                    if neighbor_id not in visited:
                        visited.add(neighbor_id)
                        queue.append((neighbor_id, path + [neighbor_id]))
            
            return None  # æœªæ‰¾åˆ°è·¯å¾„
            
        except Exception as e:
            logger.error(f"è·¯å¾„æŸ¥æ‰¾å¤±è´¥: {e}")
            return None
    
    async def _get_neighbors(self, node_id: str) -> List[str]:
        """è·å–é‚»å±…èŠ‚ç‚¹"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"neighbors:{node_id}"
            if cache_key in self.search_cache:
                return self.search_cache[cache_key]
            
            cursor = self.conn.execute("""
                SELECT target_id FROM graph_edges WHERE source_id = ?
                UNION
                SELECT source_id FROM graph_edges WHERE target_id = ?
            """, (node_id, node_id))
            
            neighbors = [row[0] for row in cursor.fetchall()]
            
            # ç¼“å­˜ç»“æœ
            self.search_cache[cache_key] = neighbors
            
            return neighbors
            
        except Exception as e:
            logger.error(f"è·å–é‚»å±…å¤±è´¥: {e}")
            return []
    
    async def _get_nodes_batch(self, node_ids: List[str]) -> List[GraphNode]:
        """æ‰¹é‡è·å–èŠ‚ç‚¹"""
        try:
            nodes = []
            uncached_ids = []
            
            # å…ˆä»ç¼“å­˜è·å–
            for node_id in node_ids:
                if node_id in self.node_cache:
                    nodes.append(self.node_cache[node_id])
                    self.performance_stats["cache_hits"] += 1
                else:
                    uncached_ids.append(node_id)
            
            # æ‰¹é‡æŸ¥è¯¢æœªç¼“å­˜çš„èŠ‚ç‚¹
            if uncached_ids:
                placeholders = ','.join(['?'] * len(uncached_ids))
                cursor = self.conn.execute(f"""
                    SELECT id, content, node_type, importance, properties, 
                           created_time, last_accessed, access_count
                    FROM graph_nodes WHERE id IN ({placeholders})
                """, uncached_ids)
                
                for row in cursor.fetchall():
                    node = GraphNode(
                        id=row[0],
                        content=row[1],
                        node_type=row[2],
                        importance=row[3],
                        properties=json.loads(row[4]) if row[4] else {},
                        created_time=row[5],
                        last_accessed=row[6],
                        access_count=row[7]
                    )
                    nodes.append(node)
                    
                    # ç¼“å­˜èŠ‚ç‚¹
                    self.node_cache[node.id] = node
            
            return nodes
            
        except Exception as e:
            logger.error(f"æ‰¹é‡è·å–èŠ‚ç‚¹å¤±è´¥: {e}")
            return []
    
    async def advanced_search(self, query: str, entities: List[str], 
                            search_strategy: str = "hybrid") -> SearchResult:
        """é«˜çº§æœç´¢"""
        try:
            start_time = time.time()
            
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = hashlib.md5(f"{query}:{':'.join(entities)}:{search_strategy}".encode()).hexdigest()
            
            # æ£€æŸ¥æœç´¢ç¼“å­˜
            if cache_key in self.search_cache:
                logger.debug("ğŸ¯ æœç´¢ç¼“å­˜å‘½ä¸­")
                return self.search_cache[cache_key]
            
            if search_strategy == "entity_first":
                result = await self._entity_first_search(query, entities)
            elif search_strategy == "importance_first":
                result = await self._importance_first_search(query, entities)
            elif search_strategy == "path_based":
                result = await self._path_based_search(query, entities)
            else:  # hybrid
                result = await self._hybrid_search(query, entities)
            
            result.search_time = time.time() - start_time
            
            # ç¼“å­˜ç»“æœ
            self.search_cache[cache_key] = result
            
            self._update_performance_stats(result.search_time, len(result.nodes))
            
            return result
            
        except Exception as e:
            logger.error(f"é«˜çº§æœç´¢å¤±è´¥: {e}")
            return SearchResult([], [], [], 0.0, 0.0)
    
    async def _entity_first_search(self, query: str, entities: List[str]) -> SearchResult:
        """å®ä½“ä¼˜å…ˆæœç´¢"""
        primary_nodes = await self.search_nodes_by_entities(entities, max_results=20)
        
        # æ‰©å±•æœç´¢
        expanded_nodes = []
        reasoning_path = [f"æ‰¾åˆ° {len(primary_nodes)} ä¸ªç›´æ¥åŒ¹é…çš„èŠ‚ç‚¹"]
        
        for node in primary_nodes[:5]:  # åªä»å‰5ä¸ªèŠ‚ç‚¹æ‰©å±•
            neighbors = await self._get_neighbors(node.id)
            neighbor_nodes = await self._get_nodes_batch(neighbors[:3])  # æ¯ä¸ªèŠ‚ç‚¹æœ€å¤š3ä¸ªé‚»å±…
            expanded_nodes.extend(neighbor_nodes)
            
            if neighbor_nodes:
                reasoning_path.append(f"ä»èŠ‚ç‚¹ '{node.content}' æ‰©å±•æ‰¾åˆ° {len(neighbor_nodes)} ä¸ªç›¸å…³èŠ‚ç‚¹")
        
        all_nodes = primary_nodes + expanded_nodes
        all_edges = await self._get_edges_between_nodes([node.id for node in all_nodes])
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = min(1.0, len(primary_nodes) * 0.2 + len(expanded_nodes) * 0.1)
        
        return SearchResult(all_nodes, all_edges, reasoning_path, confidence, 0.0)
    
    async def _importance_first_search(self, query: str, entities: List[str]) -> SearchResult:
        """é‡è¦æ€§ä¼˜å…ˆæœç´¢"""
        high_importance_nodes = await self.search_by_importance(min_importance=0.8, max_results=10)
        
        # è¿‡æ»¤ä¸æŸ¥è¯¢ç›¸å…³çš„èŠ‚ç‚¹
        relevant_nodes = []
        reasoning_path = ["æŒ‰é‡è¦æ€§æœç´¢é«˜ä»·å€¼èŠ‚ç‚¹"]
        
        for node in high_importance_nodes:
            # ç®€å•çš„ç›¸å…³æ€§æ£€æŸ¥
            if any(entity.lower() in node.content.lower() for entity in entities):
                relevant_nodes.append(node)
                reasoning_path.append(f"é«˜é‡è¦æ€§èŠ‚ç‚¹: '{node.content}' (é‡è¦æ€§: {node.importance:.2f})")
        
        edges = await self._get_edges_between_nodes([node.id for node in relevant_nodes])
        confidence = len(relevant_nodes) * 0.15
        
        return SearchResult(relevant_nodes, edges, reasoning_path, confidence, 0.0)
    
    async def _path_based_search(self, query: str, entities: List[str]) -> SearchResult:
        """è·¯å¾„åŸºç¡€æœç´¢"""
        entity_nodes = await self.search_nodes_by_entities(entities, max_results=10)
        
        if len(entity_nodes) < 2:
            return SearchResult(entity_nodes, [], ["å®ä½“èŠ‚ç‚¹ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè·¯å¾„æœç´¢"], 0.1, 0.0)
        
        # æŸ¥æ‰¾èŠ‚ç‚¹é—´è·¯å¾„
        paths = []
        reasoning_path = ["æŸ¥æ‰¾å®ä½“èŠ‚ç‚¹é—´çš„è¿æ¥è·¯å¾„"]
        
        for i, node1 in enumerate(entity_nodes[:3]):
            for node2 in entity_nodes[i+1:i+3]:  # é™åˆ¶è·¯å¾„æ•°é‡
                path = await self.find_path(node1.id, node2.id, max_depth=3)
                if path:
                    paths.append(path)
                    reasoning_path.append(f"æ‰¾åˆ°è·¯å¾„: {node1.content} â†’ {node2.content}")
        
        # æ”¶é›†è·¯å¾„ä¸Šçš„æ‰€æœ‰èŠ‚ç‚¹
        path_node_ids = set()
        for path in paths:
            path_node_ids.update(path)
        
        path_nodes = await self._get_nodes_batch(list(path_node_ids))
        path_edges = await self._get_edges_between_nodes(list(path_node_ids))
        
        confidence = min(1.0, len(paths) * 0.3)
        
        return SearchResult(path_nodes, path_edges, reasoning_path, confidence, 0.0)
    
    async def _hybrid_search(self, query: str, entities: List[str]) -> SearchResult:
        """æ··åˆæœç´¢ç­–ç•¥"""
        # å¹¶è¡Œæ‰§è¡Œå¤šç§æœç´¢ç­–ç•¥
        tasks = [
            self._entity_first_search(query, entities),
            self._importance_first_search(query, entities),
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆå¹¶ç»“æœ
        all_nodes = []
        all_edges = []
        reasoning_path = ["ä½¿ç”¨æ··åˆæœç´¢ç­–ç•¥"]
        total_confidence = 0.0
        
        for i, result in enumerate(results):
            if isinstance(result, SearchResult):
                all_nodes.extend(result.nodes)
                all_edges.extend(result.edges)
                reasoning_path.extend([f"ç­–ç•¥{i+1}: " + line for line in result.reasoning_path])
                total_confidence += result.confidence
        
        # å»é‡
        seen_node_ids = set()
        unique_nodes = []
        for node in all_nodes:
            if node.id not in seen_node_ids:
                unique_nodes.append(node)
                seen_node_ids.add(node.id)
        
        # æŒ‰é‡è¦æ€§æ’åº
        unique_nodes.sort(key=lambda x: x.importance, reverse=True)
        
        return SearchResult(unique_nodes[:20], all_edges, reasoning_path, 
                          min(1.0, total_confidence), 0.0)
    
    async def _get_edges_between_nodes(self, node_ids: List[str]) -> List[GraphEdge]:
        """è·å–èŠ‚ç‚¹é—´çš„è¾¹"""
        try:
            if not node_ids:
                return []
            
            placeholders = ','.join(['?'] * len(node_ids))
            cursor = self.conn.execute(f"""
                SELECT id, source_id, target_id, edge_type, weight, confidence, properties, created_time
                FROM graph_edges 
                WHERE source_id IN ({placeholders}) AND target_id IN ({placeholders})
            """, node_ids + node_ids)
            
            edges = []
            for row in cursor.fetchall():
                edge = GraphEdge(
                    id=row[0],
                    source_id=row[1],
                    target_id=row[2],
                    edge_type=row[3],
                    weight=row[4],
                    confidence=row[5],
                    properties=json.loads(row[6]) if row[6] else {},
                    created_time=row[7]
                )
                edges.append(edge)
            
            return edges
            
        except Exception as e:
            logger.error(f"è·å–è¾¹å¤±è´¥: {e}")
            return []
    
    def _update_performance_stats(self, search_time: float, result_count: int):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats["total_searches"] += 1
        total_time = self.performance_stats["avg_search_time"] * (self.performance_stats["total_searches"] - 1)
        self.performance_stats["avg_search_time"] = (total_time + search_time) / self.performance_stats["total_searches"]
    
    async def update_node_access(self, node_id: str):
        """æ›´æ–°èŠ‚ç‚¹è®¿é—®ä¿¡æ¯"""
        try:
            current_time = time.time()
            self.conn.execute("""
                UPDATE graph_nodes 
                SET last_accessed = ?, access_count = access_count + 1
                WHERE id = ?
            """, (current_time, node_id))
            
            # æ›´æ–°ç¼“å­˜
            if node_id in self.node_cache:
                node = self.node_cache[node_id]
                node.last_accessed = current_time
                node.access_count += 1
            
        except Exception as e:
            logger.error(f"æ›´æ–°èŠ‚ç‚¹è®¿é—®å¤±è´¥: {e}")
    
    async def cleanup_low_value_nodes(self, min_importance: float = 0.3, 
                                    min_access_count: int = 1, days_threshold: int = 30):
        """æ¸…ç†ä½ä»·å€¼èŠ‚ç‚¹"""
        try:
            cutoff_time = time.time() - (days_threshold * 24 * 3600)
            
            cursor = self.conn.execute("""
                SELECT id FROM graph_nodes 
                WHERE importance < ? AND access_count < ? AND created_time < ?
            """, (min_importance, min_access_count, cutoff_time))
            
            low_value_ids = [row[0] for row in cursor.fetchall()]
            
            if low_value_ids:
                # åˆ é™¤èŠ‚ç‚¹å’Œç›¸å…³è¾¹
                placeholders = ','.join(['?'] * len(low_value_ids))
                
                self.conn.execute(f"DELETE FROM graph_edges WHERE source_id IN ({placeholders}) OR target_id IN ({placeholders})", 
                                low_value_ids + low_value_ids)
                
                self.conn.execute(f"DELETE FROM graph_nodes WHERE id IN ({placeholders})", low_value_ids)
                
                self.conn.execute(f"DELETE FROM entity_node_index WHERE node_id IN ({placeholders})", low_value_ids)
                
                # æ¸…ç†ç¼“å­˜
                for node_id in low_value_ids:
                    if node_id in self.node_cache:
                        del self.node_cache[node_id]
                
                self.conn.commit()
                
                # é‡å»ºç´¢å¼•
                self._build_indexes()
                
                logger.info(f"ğŸ§¹ æ¸…ç†äº† {len(low_value_ids)} ä¸ªä½ä»·å€¼èŠ‚ç‚¹")
                
                return len(low_value_ids)
            
            return 0
            
        except Exception as e:
            logger.error(f"æ¸…ç†ä½ä»·å€¼èŠ‚ç‚¹å¤±è´¥: {e}")
            return 0
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        cache_hit_rate = (self.performance_stats["cache_hits"] / 
                         max(self.performance_stats["total_searches"], 1))
        
        return {
            **self.performance_stats,
            "cache_hit_rate": cache_hit_rate,
            "cache_sizes": {
                "node_cache": len(self.node_cache),
                "edge_cache": len(self.edge_cache),
                "search_cache": len(self.search_cache)
            },
            "index_sizes": {
                "entity_index": len(self.entity_index),
                "type_index": len(self.type_index),
                "importance_index": len(self.importance_index)
            }
        }
    
    def close(self):
        """å…³é—­å¼•æ“"""
        try:
            self.executor.shutdown(wait=True)
            logger.info("âš¡ ä¼˜åŒ–å›¾è°±å¼•æ“å·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­å›¾è°±å¼•æ“å¤±è´¥: {e}")