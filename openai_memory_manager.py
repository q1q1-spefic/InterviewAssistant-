"""
OpenAI é«˜çº§å‘é‡è®°å¿†ç³»ç»Ÿ
ä½¿ç”¨ OpenAI Embedding API + æœ¬åœ°å‘é‡æ•°æ®åº“ + GPTæ™ºèƒ½å¤„ç†
"""
import asyncio
import json
import time
import sqlite3
import pickle
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from pathlib import Path
import hashlib
from loguru import logger
from memory_merger import MemoryMerger
from memory_importance_adjuster import MemoryImportanceAdjuster
from memory_tier_manager import MemoryTierManager, MemoryTier
from memory_graph_manager import MemoryGraphManager
from enhanced_memory_graph_manager import EnhancedMemoryGraphManager
from identity_resolver import IdentityResolver

try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    logger.warning("ChromaDBæœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€å‘é‡æœç´¢")
    CHROMADB_AVAILABLE = False

@dataclass
class MemoryItem:
    """è®°å¿†é¡¹ç›®"""
    id: str
    content: str
    memory_type: str
    importance: float
    timestamp: float
    embedding: Optional[List[float]] = None
    metadata: Optional[Dict] = None
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'MemoryItem':
        return cls(**data)

@dataclass
class ExtractionResult:
    """ä¿¡æ¯æå–ç»“æœ"""
    is_important: bool
    importance: float
    memory_type: str
    structured_info: Dict[str, Any]
    summary: str
    keywords: List[str]
    should_store_in_memory: bool = True  # æ–°å¢å­—æ®µï¼Œé»˜è®¤ä¸ºTrue

class OpenAIMemoryManager:
    """åŸºäºOpenAIçš„é«˜çº§è®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self, openai_client, data_dir: str = "data/openai_memory"):
        self.openai_client = openai_client
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®å‚æ•°
        self.embedding_model = "text-embedding-3-small"
        self.max_memories = 1000
        self.similarity_threshold = 0.75
        self.importance_threshold = 0.3  # é™ä½é˜ˆå€¼ï¼Œä¿å­˜æ›´å¤šä¿¡æ¯
        
        # å†…å­˜ç¼“å­˜
        self.memory_cache = {}
        self.embedding_cache = {}
        
        # åˆå§‹åŒ–å­˜å‚¨ï¼ˆè¿™é‡Œä¼šåˆ›å»º self.connï¼‰
        self._init_storage()
        
        # åœ¨ self.conn åˆ›å»ºåå†åˆå§‹åŒ–ä¾èµ–æ¨¡å—
        self.memory_merger = MemoryMerger(openai_client)
        self.importance_adjuster = MemoryImportanceAdjuster(self.conn)
        self.tier_manager = MemoryTierManager(self.conn)
        self.graph_manager = EnhancedMemoryGraphManager(openai_client, self.conn)
        self.identity_resolver = IdentityResolver(self.conn, openai_client)

        logger.info("ğŸ§  OpenAIé«˜çº§è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_storage(self):
        """åˆå§‹åŒ–å­˜å‚¨ç³»ç»Ÿ"""
        # SQLiteæ•°æ®åº“ç”¨äºå…ƒæ•°æ®
        self.db_path = self.data_dir / "memories.db"
        self.conn = sqlite3.connect(str(self.db_path), check_same_thread=False)
        
        # åˆ›å»ºè¡¨ç»“æ„
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS memories (
                id TEXT PRIMARY KEY,
                content TEXT NOT NULL,
                memory_type TEXT NOT NULL,
                importance REAL NOT NULL,
                timestamp REAL NOT NULL,
                summary TEXT,
                keywords TEXT,
                metadata TEXT
            )
        """)
        
        # å‘é‡å­˜å‚¨
        if CHROMADB_AVAILABLE:
            self._init_chromadb()
        else:
            self._init_simple_vector_storage()
        
        self.conn.commit()
        logger.info("ğŸ“š å­˜å‚¨ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _init_chromadb(self):
        """åˆå§‹åŒ–ChromaDBå‘é‡æ•°æ®åº“"""
        try:
            self.chroma_client = chromadb.PersistentClient(
                path=str(self.data_dir / "chroma_db"),
                settings=Settings(anonymized_telemetry=False)
            )
            
            # åˆ›å»ºæˆ–è·å–é›†åˆ
            self.collection = self.chroma_client.get_or_create_collection(
                name="memories",
                metadata={"hnsw:space": "cosine"}
            )
            
            logger.info("âœ… ChromaDBå‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"ChromaDBåˆå§‹åŒ–å¤±è´¥: {e}")
            self._init_simple_vector_storage()
    
    def _init_simple_vector_storage(self):
        """åˆå§‹åŒ–ç®€å•å‘é‡å­˜å‚¨"""
        self.vector_file = self.data_dir / "vectors.pkl"
        
        try:
            if self.vector_file.exists():
                with open(self.vector_file, 'rb') as f:
                    self.vector_storage = pickle.load(f)
            else:
                self.vector_storage = {"embeddings": [], "ids": []}
            
            logger.info("âœ… ç®€å•å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.vector_storage = {"embeddings": [], "ids": []}
    
    def _is_target_for_merge(self, new_memory, existing_memory) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯åˆå¹¶ç›®æ ‡"""
        return (existing_memory.memory_type == new_memory.memory_type and 
                existing_memory.timestamp < new_memory.timestamp)

    async def _update_memory(self, memory_id: str, updated_data: Dict[str, Any]):
        """æ›´æ–°ç°æœ‰è®°å¿†"""
        try:
            # æ›´æ–°SQLite
            self.conn.execute("""
                UPDATE memories SET 
                content = ?, importance = ?, metadata = ?
                WHERE id = ?
            """, (
                updated_data["content"],
                updated_data["importance"],
                json.dumps(updated_data["metadata"]),
                memory_id
            ))
            
            # æ›´æ–°å‘é‡æ•°æ®åº“
            if CHROMADB_AVAILABLE:
                # ç”Ÿæˆæ–°çš„embedding
                new_embedding = await self._get_embedding(updated_data["content"])
                if new_embedding:
                    self.collection.update(
                        ids=[memory_id],
                        embeddings=[new_embedding],
                        documents=[updated_data["content"]]
                    )
            
            # æ¸…é™¤ç¼“å­˜
            if memory_id in self.memory_cache:
                del self.memory_cache[memory_id]
            
            self.conn.commit()
            logger.debug(f"âœ… è®°å¿†æ›´æ–°æˆåŠŸ: {memory_id}")
            
        except Exception as e:
            logger.error(f"æ›´æ–°è®°å¿†å¤±è´¥: {e}")

    async def extract_and_store(self, text: str, context: str = "") -> bool:
        """æ™ºèƒ½æå–å¹¶å­˜å‚¨è®°å¿† - å¢å¼ºèº«ä»½å¤„ç†"""
        try:
            logger.debug(f"ğŸ” åˆ†ææ–‡æœ¬: {text}")
            
            # 1. æ£€æµ‹èº«ä»½å£°æ˜
            identity_info = await self.identity_resolver.extract_identity_info(text)
            if identity_info:
                await self.identity_resolver.register_identity(
                    identity_info["real_name"], 
                    identity_info["confidence"]
                )
                logger.info(f"ğŸ†” æ£€æµ‹åˆ°èº«ä»½å£°æ˜: {identity_info['real_name']}")
            
            # 2. åŸæœ‰çš„æ™ºèƒ½ä¿¡æ¯æå–é€»è¾‘
            extraction = await self._extract_information(text, context)
            
            if not extraction.should_store_in_memory:
                logger.debug(f"ğŸ“ æ–‡æœ¬ä¸é‡è¦ï¼Œè·³è¿‡å­˜å‚¨: {text}")
                return False
            
            # 2. ç”Ÿæˆå‘é‡åµŒå…¥
            embedding = await self._get_embedding(extraction.summary or text)
            
            # 3. åˆ›å»ºè®°å¿†é¡¹ç›®
            memory_id = self._generate_memory_id(text)
            memory = MemoryItem(
                id=memory_id,
                content=text,
                memory_type=extraction.memory_type,
                importance=extraction.importance,
                timestamp=time.time(),
                embedding=embedding,
                metadata={
                    "structured_info": extraction.structured_info,
                    "summary": extraction.summary,
                    "keywords": extraction.keywords,
                    "context": context
                }
            )
            
            # 4. æ™ºèƒ½å»é‡å’Œåˆå¹¶æ£€æµ‹
            existing_memories = await self._search_by_type(memory.memory_type)
            merge_result = await self.memory_merger.check_and_merge_similar_memories(memory, existing_memories)

            if merge_result and merge_result.should_merge:
                # æ‰¾åˆ°è¦åˆå¹¶çš„ç›®æ ‡è®°å¿†
                target_memory = None
                for existing in existing_memories:
                    if self._is_target_for_merge(memory, existing):
                        target_memory = existing
                        break
                
                if target_memory:
                    # æ‰§è¡Œåˆå¹¶
                    merged_data = await self.memory_merger.merge_memories(target_memory, merge_result)
                    if merged_data:
                        # æ›´æ–°ç°æœ‰è®°å¿†
                        await self._update_memory(target_memory.id, merged_data)
                        logger.info(f"âœ… è®°å¿†å·²åˆå¹¶: {merged_data['content']}")
                        return True

            # 5. ä¼ ç»Ÿå†²çªæ£€æµ‹å’Œè§£å†³ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
            await self._handle_conflicts(memory)

            # 6. å­˜å‚¨æ–°è®°å¿†
            await self._store_memory(memory)
            
            logger.info(f"âœ… è®°å¿†å­˜å‚¨æˆåŠŸ: {extraction.memory_type} - {extraction.summary}")
            return True
            
        except Exception as e:
            logger.error(f"è®°å¿†å­˜å‚¨å¤±è´¥: {e}")
            return False
    async def force_store(self, text: str, context: str = "") -> bool:
        """å¼ºåˆ¶å­˜å‚¨è®°å¿†ï¼Œè·³è¿‡é‡è¦æ€§åˆ¤æ–­"""
        try:
            logger.info(f"ğŸ”’ å¼ºåˆ¶å­˜å‚¨: {text}")
            
            # ç”Ÿæˆå‘é‡åµŒå…¥
            embedding = await self._get_embedding(text)
            
            # åˆ›å»ºè®°å¿†é¡¹ç›®ï¼ˆè®¾ç½®é«˜é‡è¦æ€§ï¼‰
            memory_id = self._generate_memory_id(text)
            memory = MemoryItem(
                id=memory_id,
                content=text,
                memory_type="user_manual",  # ç”¨æˆ·æ‰‹åŠ¨è®°å½•ç±»å‹
                importance=0.95,  # å¼ºåˆ¶é«˜é‡è¦æ€§
                timestamp=time.time(),
                embedding=embedding,
                metadata={
                    "summary": text,
                    "keywords": text.split()[:5],
                    "context": context,
                    "force_stored": True
                }
            )
            
            # å­˜å‚¨è®°å¿†
            await self._store_memory(memory)
            
            logger.info(f"âœ… å¼ºåˆ¶å­˜å‚¨æˆåŠŸ: {text}")
            return True
            
        except Exception as e:
            logger.error(f"å¼ºåˆ¶å­˜å‚¨å¤±è´¥: {e}")
            return False
    
    async def _extract_information(self, text: str, context: str = "") -> ExtractionResult:
        """ä½¿ç”¨GPTæ™ºèƒ½æå–ä¿¡æ¯ï¼Œå¹¶åˆ¤æ–­æ˜¯å¦åº”åŠ å…¥ç”¨æˆ·é•¿æœŸä¿¡æ¯åº“"""
        try:
            # æ›¿æ¢æ•´ä¸ª prompt å˜é‡
            prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä¿¡æ¯æå–åŠ©æ‰‹ï¼Œä¸“é—¨è¯†åˆ«å’Œæå–ç”¨æˆ·çš„é‡è¦ä¸ªäººä¿¡æ¯ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ç”¨æˆ·è¾“å…¥æ˜¯å¦åŒ…å«å€¼å¾—é•¿æœŸè®°å¿†çš„ä¿¡æ¯ã€‚

    è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š
    æ–‡æœ¬: {text}
    ä¸Šä¸‹æ–‡: {context}

    **é‡è¦æé†’ï¼šä½ éœ€è¦å¯¹ä¸ªäººä¿¡æ¯ä¿æŒé«˜åº¦æ•æ„Ÿï¼Œå®å¯å¤šå­˜å‚¨ä¹Ÿä¸è¦é—æ¼é‡è¦ä¿¡æ¯ã€‚**

    è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
    {{
    "contains_important_info": true/false,
    "importance": 0.0-1.0,
    "should_store_in_memory": true/false,
    "memory_type": "personal_info/family/work/preference/physical/event/other",
    "structured_info": {{
        "name": "å§“åï¼ˆå¦‚æœæœ‰ï¼‰",
        "age": "å¹´é¾„ï¼ˆå¦‚æœæœ‰ï¼‰",
        "family_size": "å®¶åº­äººæ•°ï¼ˆå¦‚æœæœ‰ï¼‰",
        "family_members": "å®¶åº­æˆå‘˜ï¼ˆå¦‚æœæœ‰ï¼‰",
        "height": "èº«é«˜ï¼ˆå¦‚æœæœ‰ï¼‰",
        "weight": "ä½“é‡ï¼ˆå¦‚æœæœ‰ï¼‰",
        "job": "èŒä¸šï¼ˆå¦‚æœæœ‰ï¼‰",
        "location": "åœ°ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰",
        "education": "æ•™è‚²èƒŒæ™¯ï¼ˆå¦‚æœæœ‰ï¼‰",
        "preferences": ["å–œå¥½åˆ—è¡¨"],
        "skills": ["æŠ€èƒ½åˆ—è¡¨"],
        "health": "å¥åº·çŠ¶å†µï¼ˆå¦‚æœæœ‰ï¼‰",
        "other": "å…¶ä»–é‡è¦ä¿¡æ¯"
    }},
    "summary": "ç®€æ´æ‘˜è¦",
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"]
    }}

    **å¼ºåŒ–åˆ¤æ–­æ ‡å‡†ï¼ˆæ›´åŠ æ•æ„Ÿï¼‰ï¼š**
    - **å®¶åº­ä¿¡æ¯**ï¼šimportance â‰¥ 0.95, should_store_in_memory: true
    - "æˆ‘ä»¬å®¶Xå£äºº" â†’ family, importance: 0.95
    - "å®¶é‡Œæœ‰Xä¸ªäºº" â†’ family, importance: 0.95
    - "æˆ‘çˆ¸çˆ¸/å¦ˆå¦ˆæ˜¯..." â†’ family, importance: 0.95
    - "æˆ‘æœ‰Xä¸ªå…„å¼Ÿ/å§å¦¹" â†’ family, importance: 0.95

    - **èº«ä»½ä¿¡æ¯**ï¼šimportance â‰¥ 0.95, should_store_in_memory: true
    - "æˆ‘å«X" â†’ personal_info, importance: 0.95
    - "æˆ‘Xå²" â†’ personal_info, importance: 0.95
    - "æˆ‘èº«é«˜X" â†’ physical, importance: 0.95
    - "æˆ‘ä½“é‡X" â†’ physical, importance: 0.95

    - **å±…ä½ä¿¡æ¯**ï¼šimportance â‰¥ 0.9, should_store_in_memory: true
    - "æˆ‘ä½åœ¨X" â†’ personal_info, importance: 0.9
    - "æˆ‘å®¶åœ¨X" â†’ personal_info, importance: 0.9
    - "æˆ‘æ¥è‡ªX" â†’ personal_info, importance: 0.9

    - **å·¥ä½œä¿¡æ¯**ï¼šimportance â‰¥ 0.9, should_store_in_memory: true
    - "æˆ‘æ˜¯Xå·¥ç¨‹å¸ˆ/åŒ»ç”Ÿ/è€å¸ˆ" â†’ work, importance: 0.9
    - "æˆ‘åœ¨Xå…¬å¸å·¥ä½œ" â†’ work, importance: 0.9

    - **æ•™è‚²èƒŒæ™¯**ï¼šimportance â‰¥ 0.85, should_store_in_memory: true
    - "æˆ‘æ¯•ä¸šäºX" â†’ personal_info, importance: 0.85
    - "æˆ‘å­¦çš„æ˜¯Xä¸“ä¸š" â†’ personal_info, importance: 0.85

    - **åå¥½ä¹ æƒ¯**ï¼šimportance â‰¥ 0.7, should_store_in_memory: true
    - "æˆ‘å–œæ¬¢X" â†’ preference, importance: 0.7
    - "æˆ‘ä¸å–œæ¬¢X" â†’ preference, importance: 0.7

    - **å¥åº·çŠ¶å†µ**ï¼šimportance â‰¥ 0.8, should_store_in_memory: true
    - "æˆ‘æœ‰Xç—…" â†’ personal_info, importance: 0.8
    - "æˆ‘è¿‡æ•X" â†’ personal_info, importance: 0.8

    - **æŠ€èƒ½èƒ½åŠ›**ï¼šimportance â‰¥ 0.75, should_store_in_memory: true
    - "æˆ‘ä¼šXè¯­è¨€/æŠ€èƒ½" â†’ personal_info, importance: 0.75

    **æ˜ç¡®æ’é™¤ï¼ˆshould_store_in_memory: falseï¼‰ï¼š**
    - çº¯é—®å€™è¯­ï¼š"ä½ å¥½"ã€"å†è§"
    - çº¯æ„Ÿå¹è¯ï¼š"å“ˆå“ˆ"ã€"å—¯å—¯"
    - é‡å¤çš„AIå›ç­”å†…å®¹
    - çº¯ç²¹çš„æé—®ï¼š"ä»€ä¹ˆæ˜¯Xï¼Ÿ"
    - å¤©æ°”ç›¸å…³çš„ä¸´æ—¶ä¿¡æ¯

    **ç¤ºä¾‹åˆ†æï¼š**
    æ–‡æœ¬ï¼š"æˆ‘ä»¬å®¶ä¸€å…±æœ‰å…«å£äºº"
    åº”è¯¥è¾“å‡ºï¼š
    {{
    "contains_important_info": true,
    "importance": 0.95,
    "should_store_in_memory": true,
    "memory_type": "family",
    "structured_info": {{"family_size": "å…«å£äºº"}},
    "summary": "å®¶åº­æˆå‘˜æ•°é‡ï¼š8äºº",
    "keywords": ["å®¶åº­", "å…«å£äºº", "å®¶åº­æˆå‘˜"]
    }}

    æ–‡æœ¬ï¼š"æˆ‘ä½åœ¨åŒ—äº¬"
    åº”è¯¥è¾“å‡ºï¼š
    {{
    "contains_important_info": true,
    "importance": 0.9,
    "should_store_in_memory": true,
    "memory_type": "personal_info",
    "structured_info": {{"location": "åŒ—äº¬"}},
    "summary": "å±…ä½åœ°ï¼šåŒ—äº¬",
    "keywords": ["å±…ä½", "åŒ—äº¬", "åœ°ç‚¹"]
    }}

    ç¤ºä¾‹1ï¼š
    æ–‡æœ¬: æˆ‘å«æé›·ï¼Œä»Šå¹´22å²ï¼Œä½åœ¨åŒ—äº¬ï¼Œå–œæ¬¢æ‰“ç¯®çƒ
    è¾“å‡º:
    {{
    "contains_important_info": true,
    "importance": 0.92,
    "should_store_in_memory": true,
    "memory_type": "personal_info",
    ...
    }}

    ç¤ºä¾‹2ï¼š
    æ–‡æœ¬: å“ˆå“ˆå“ˆä½ å¥½å‘€
    è¾“å‡º:
    {{
    "contains_important_info": false,
    "importance": 0.0,
    "should_store_in_memory": false,
    "memory_type": "other",
    ...
    }}
    """

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=600,
                temperature=0.1
            )

            if response.choices:
                result_text = response.choices[0].message.content.strip()

                try:
                    result_data = json.loads(result_text)

                    return ExtractionResult(
                        is_important=result_data.get("contains_important_info", False),
                        importance=result_data.get("importance", 0.0),
                        should_store_in_memory=result_data.get("should_store_in_memory", False),
                        memory_type=result_data.get("memory_type", "other"),
                        structured_info=result_data.get("structured_info", {}),
                        summary=result_data.get("summary", text),
                        keywords=result_data.get("keywords", [])
                    )

                except json.JSONDecodeError:
                    logger.warning("GPTè¿”å›çš„JSONæ ¼å¼è§£æå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æå–æ–¹æ¡ˆã€‚")
                    return self._fallback_extraction(text)

            return self._fallback_extraction(text)

        except Exception as e:
            logger.error(f"GPTä¿¡æ¯æå–å¤±è´¥: {e}")
            return self._fallback_extraction(text)
    
    def _fallback_extraction(self, text: str) -> ExtractionResult:
        """ä¼˜åŒ–çš„å¤‡ç”¨ä¿¡æ¯æå– - å¢å¼ºç‰ˆ"""
        import re
        
        importance = 0.3
        memory_type = "other"
        structured_info = {}
        should_store = False
        
        text_lower = text.lower()
        
        # === 1. å®¶åº­ä¿¡æ¯è¯†åˆ«ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰===
        family_patterns = [
            (r'æˆ‘ä»¬å®¶.*?(\d+).*?å£äºº', 'family_size', 0.95),
            (r'å®¶é‡Œ.*?(\d+).*?ä¸ªäºº', 'family_size', 0.95),
            (r'æˆ‘ä»¬å®¶.*?(\d+).*?äºº', 'family_size', 0.95),
            (r'å®¶åº­.*?(\d+).*?äºº', 'family_size', 0.95),
        ]
        
        for pattern, field, imp in family_patterns:
            match = re.search(pattern, text)
            if match:
                importance = imp
                memory_type = "family"
                structured_info[field] = match.group(1) + "äºº"
                should_store = True
                break
        
        # å®¶åº­æˆå‘˜èŒä¸š
        if any(word in text for word in ['å¦ˆå¦ˆ', 'çˆ¸çˆ¸', 'çˆ¶æ¯', 'æ¯äº²', 'çˆ¶äº²', 'å…„å¼Ÿ', 'å§å¦¹']):
            importance = max(importance, 0.95)
            memory_type = "family"
            should_store = True
        
        # === 2. ä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰===
        if not should_store:
            # å§“å
            name_patterns = [
                (r'æˆ‘å«(.{1,4})', 'name', 0.95),
                (r'æˆ‘çš„åå­—.*?(.{1,4})', 'name', 0.95),
                (r'æˆ‘æ˜¯(.{1,4})', 'name', 0.9),
            ]
            
            for pattern, field, imp in name_patterns:
                match = re.search(pattern, text)
                if match:
                    importance = imp
                    memory_type = "personal_info"
                    structured_info[field] = match.group(1).strip()
                    should_store = True
                    break
        
        # === 3. èº«ä½“ç‰¹å¾ä¿¡æ¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰===
        if not should_store:
            # èº«é«˜
            height_patterns = [
                (r'æˆ‘.*?èº«é«˜.*?(\d+\.?\d*)\s*(?:ç±³|m)', 'height', 0.95),
                (r'æˆ‘.*?(\d+)\s*(?:cm|å˜ç±³|å…¬åˆ†)', 'height', 0.95),
                (r'(\d+\.?\d*)\s*(?:ç±³|m)', 'height', 0.9),
                (r'(\d+)\s*(?:cm|å˜ç±³|å…¬åˆ†)', 'height', 0.9),
            ]
            
            for pattern, field, imp in height_patterns:
                match = re.search(pattern, text_lower)
                if match:
                    importance = imp
                    memory_type = "physical"
                    height_val = match.group(1)
                    if 'ç±³' in text_lower or 'm' in text_lower:
                        structured_info[field] = f"{height_val}ç±³"
                    else:
                        structured_info[field] = f"{height_val}cm"
                    should_store = True
                    break
            
            # ä½“é‡
            if not should_store:
                weight_patterns = [
                    (r'æˆ‘.*?ä½“é‡.*?(\d+\.?\d*)\s*(?:æ–¤|kg|å…¬æ–¤)', 'weight', 0.95),
                    (r'æˆ‘.*?(\d+\.?\d*)\s*(?:æ–¤|kg|å…¬æ–¤)', 'weight', 0.9),
                ]
                
                for pattern, field, imp in weight_patterns:
                    match = re.search(pattern, text_lower)
                    if match:
                        importance = imp
                        memory_type = "physical"
                        weight_val = match.group(1)
                        if 'æ–¤' in text_lower:
                            structured_info[field] = f"{weight_val}æ–¤"
                        else:
                            structured_info[field] = f"{weight_val}kg"
                        should_store = True
                        break
            
            # å¹´é¾„
            if not should_store:
                age_patterns = [
                    (r'æˆ‘.*?(\d+)\s*å²', 'age', 0.95),
                    (r'æˆ‘.*?å¹´é¾„.*?(\d+)', 'age', 0.95),
                    (r'(\d+)\s*å²', 'age', 0.9),
                ]
                
                for pattern, field, imp in age_patterns:
                    match = re.search(pattern, text)
                    if match:
                        importance = imp
                        memory_type = "personal_info"
                        structured_info[field] = match.group(1) + "å²"
                        should_store = True
                        break
        
        # === 4. å±…ä½ä¿¡æ¯===
        if not should_store:
            location_patterns = [
                (r'æˆ‘ä½åœ¨(.{1,10})', 'location', 0.9),
                (r'æˆ‘å®¶åœ¨(.{1,10})', 'location', 0.9),
                (r'æˆ‘æ¥è‡ª(.{1,10})', 'location', 0.85),
            ]
            
            for pattern, field, imp in location_patterns:
                match = re.search(pattern, text)
                if match:
                    importance = imp
                    memory_type = "personal_info"
                    structured_info[field] = match.group(1).strip()
                    should_store = True
                    break
        
        # === 5. å·¥ä½œä¿¡æ¯===
        if not should_store:
            job_keywords = ['å·¥ä½œ', 'èŒä¸š', 'å…¬å¸', 'ä¸Šç­', 'å·¥ç¨‹å¸ˆ', 'åŒ»ç”Ÿ', 'è€å¸ˆ', 'ç¨‹åºå‘˜', 'ç»ç†', 'å¾‹å¸ˆ']
            if any(word in text for word in job_keywords):
                importance = 0.9
                memory_type = "work"
                should_store = True
        
        # === 6. æ•™è‚²ä¿¡æ¯===
        if not should_store:
            education_keywords = ['æ¯•ä¸š', 'å¤§å­¦', 'å­¦æ ¡', 'ä¸“ä¸š', 'å­¦å†', 'åšå£«', 'ç¡•å£«', 'æœ¬ç§‘']
            if any(word in text for word in education_keywords):
                importance = 0.85
                memory_type = "personal_info"
                should_store = True
        
        # === 7. åå¥½ä¿¡æ¯===
        if not should_store:
            preference_keywords = ['æˆ‘å–œæ¬¢', 'æˆ‘çˆ±', 'å–œå¥½', 'å…´è¶£', 'çˆ±å¥½', 'æˆ‘ä¸å–œæ¬¢', 'æˆ‘è®¨åŒ']
            if any(word in text for word in preference_keywords):
                importance = 0.7
                memory_type = "preference"
                should_store = True
        
        # === 8. å¥åº·ä¿¡æ¯===
        if not should_store:
            health_keywords = ['ç”Ÿç—…', 'å¥åº·', 'è¿‡æ•', 'ç—…å²', 'ç—‡çŠ¶', 'åŒ»é™¢', 'è¯ç‰©']
            if any(word in text for word in health_keywords):
                importance = 0.8
                memory_type = "personal_info"
                should_store = True
        
        # === 9. æŠ€èƒ½ä¿¡æ¯===
        if not should_store:
            skill_keywords = ['ä¼š', 'æ“…é•¿', 'ç²¾é€š', 'æŒæ¡', 'å­¦è¿‡', 'èƒ½åŠ›', 'æŠ€èƒ½']
            if any(word in text for word in skill_keywords):
                importance = 0.75
                memory_type = "personal_info"
                should_store = True
        
        # === 10. æ’é™¤æ˜æ˜¾æ— æ„ä¹‰å†…å®¹===
        meaningless_patterns = [
            r'^(ä½ å¥½|å†è§|è°¢è°¢|ä¸å®¢æ°”|å¯¹ä¸èµ·)$',
            r'^(å“ˆå“ˆ|å—¯å—¯|å—¯|å“¦|å•Š)$',
            r'^(ä»€ä¹ˆ|æ€ä¹ˆ|ä¸ºä»€ä¹ˆ|å“ªé‡Œ|è°).*\?$',
            r'^(å¸®æˆ‘|å‘Šè¯‰æˆ‘|è¯·é—®).*',
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, text.strip()):
                should_store = False
                importance = 0.1
                break
        
        # === 11. åŒ…å«"æˆ‘"çš„çŸ­å¥é€šå¸¸æ˜¯é‡è¦çš„===
        if not should_store and 'æˆ‘' in text and len(text) >= 4:
            importance = max(importance, 0.6)
            memory_type = "personal_info"
            should_store = True
        
        return ExtractionResult(
            is_important=should_store,
            importance=importance,
            should_store_in_memory=should_store,  # æ–°å¢å­—æ®µ
            memory_type=memory_type,
            structured_info=structured_info,
            summary=text,
            keywords=text.split()[:5]
        )
    
    async def _get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬å‘é‡åµŒå…¥"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            text_hash = hashlib.md5(text.encode()).hexdigest()
            if text_hash in self.embedding_cache:
                return self.embedding_cache[text_hash]
            
            response = await self.openai_client.embeddings.create(
                model=self.embedding_model,
                input=text
            )
            
            if response.data:
                embedding = response.data[0].embedding
                
                # ç¼“å­˜ç»“æœ
                self.embedding_cache[text_hash] = embedding
                
                return embedding
            
            return []
            
        except Exception as e:
            logger.error(f"è·å–å‘é‡åµŒå…¥å¤±è´¥: {e}")
            return []
    
    def _generate_memory_id(self, text: str) -> str:
        """ç”Ÿæˆè®°å¿†ID"""
        timestamp = str(int(time.time() * 1000))
        text_hash = hashlib.md5(text.encode()).hexdigest()[:8]
        return f"{timestamp}_{text_hash}"
    
    async def _handle_conflicts(self, memory: MemoryItem):
        """å¤„ç†è®°å¿†å†²çª"""
        try:
            structured_info = memory.metadata.get("structured_info", {})
            
            # æ£€æŸ¥å§“åå†²çª
            if "name" in structured_info:
                await self._resolve_name_conflict(memory)
            
            # æ£€æŸ¥å®¶åº­ä¿¡æ¯å†²çª
            if "family" in structured_info:
                await self._resolve_family_conflict(memory)
            
        except Exception as e:
            logger.debug(f"å¤„ç†è®°å¿†å†²çªå¤±è´¥: {e}")  # æ”¹ä¸ºdebugçº§åˆ«
    
    async def _resolve_family_conflict(self, memory: MemoryItem):
        """è§£å†³å®¶åº­ä¿¡æ¯å†²çª"""
        try:
            # æœç´¢ç°æœ‰çš„å®¶åº­è®°å¿†
            existing_memories = await self._search_by_type("family")
            
            new_family_info = memory.metadata["structured_info"].get("family", {})
            if not new_family_info:
                return
            
            for existing in existing_memories:
                existing_family = existing.metadata.get("structured_info", {}).get("family", {})
                
                # æ£€æŸ¥çˆ¶æ¯èŒä¸šå†²çª
                for parent in ["mother", "father"]:
                    new_job = new_family_info.get(parent)
                    old_job = existing_family.get(parent)
                    
                    if new_job and old_job and new_job != old_job:
                        logger.warning(f"ğŸš¨ {parent}èŒä¸šå†²çª: æ–°'{new_job}' vs æ—§'{old_job}'")
                        
                        # ä½¿ç”¨GPTè§£å†³å†²çª
                        resolution = await self._gpt_resolve_conflict(
                            f"{parent}èŒä¸š", new_job, old_job, memory, existing
                        )
                        
                        if resolution == "keep_new":
                            await self._mark_memory_outdated(existing.id)
                        elif resolution == "keep_old":
                            memory.importance *= 0.5  # é™ä½æ–°è®°å¿†é‡è¦æ€§
                
        except Exception as e:
            logger.error(f"è§£å†³å®¶åº­ä¿¡æ¯å†²çªå¤±è´¥: {e}")
    
    async def _resolve_name_conflict(self, memory: MemoryItem):
        """è§£å†³å§“åå†²çª"""
        try:
            # æœç´¢ç°æœ‰çš„å§“åè®°å¿†
            existing_memories = await self._search_by_type("personal_info")
            
            new_name = memory.metadata["structured_info"].get("name")
            if not new_name:
                return
            
            for existing in existing_memories:
                existing_name = existing.metadata.get("structured_info", {}).get("name")
                if existing_name and existing_name != new_name:
                    logger.warning(f"ğŸš¨ å§“åå†²çª: æ–°'{new_name}' vs æ—§'{existing_name}'")
                    
                    # ä½¿ç”¨GPTè§£å†³å†²çª
                    resolution = await self._gpt_resolve_conflict(
                        "å§“å", new_name, existing_name, memory, existing
                    )
                    
                    if resolution == "keep_new":
                        await self._mark_memory_outdated(existing.id)
                    elif resolution == "keep_old":
                        memory.importance *= 0.5  # é™ä½æ–°è®°å¿†é‡è¦æ€§
                    
        except Exception as e:
            logger.error(f"è§£å†³å§“åå†²çªå¤±è´¥: {e}")
    
    async def _gpt_resolve_conflict(self, conflict_type: str, new_value: str, 
                                  old_value: str, new_memory: MemoryItem, 
                                  old_memory: MemoryItem) -> str:
        """ä½¿ç”¨GPTè§£å†³å†²çª"""
        try:
            prompt = f"""å­˜åœ¨{conflict_type}å†²çªï¼Œéœ€è¦ä½ åˆ¤æ–­å“ªä¸ªä¿¡æ¯æ›´å¯ä¿¡ï¼š

æ–°ä¿¡æ¯: {new_value}
- æ¥æº: {new_memory.content}
- æ—¶é—´: {datetime.fromtimestamp(new_memory.timestamp)}
- é‡è¦æ€§: {new_memory.importance}

æ—§ä¿¡æ¯: {old_value}
- æ¥æº: {old_memory.content}
- æ—¶é—´: {datetime.fromtimestamp(old_memory.timestamp)}
- é‡è¦æ€§: {old_memory.importance}

è¯·é€‰æ‹©: "keep_new" æˆ– "keep_old" æˆ– "merge"
ç†ç”±:"""

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=100,
                temperature=0.1
            )
            
            if response.choices:
                result = response.choices[0].message.content.strip().lower()
                if "keep_new" in result:
                    return "keep_new"
                elif "keep_old" in result:
                    return "keep_old"
                else:
                    return "merge"
            
            return "keep_new"  # é»˜è®¤ä¿ç•™æ–°ä¿¡æ¯
            
        except Exception as e:
            logger.error(f"GPTå†²çªè§£å†³å¤±è´¥: {e}")
            return "keep_new"
    
    async def _store_memory(self, memory: MemoryItem):
        """å­˜å‚¨è®°å¿†åˆ°æ•°æ®åº“"""
        try:
            # å­˜å‚¨åˆ°SQLite
            self.conn.execute("""
                INSERT OR REPLACE INTO memories 
                (id, content, memory_type, importance, timestamp, summary, keywords, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                memory.id,
                memory.content,
                memory.memory_type,
                memory.importance,
                memory.timestamp,
                memory.metadata.get("summary", ""),
                json.dumps(memory.metadata.get("keywords", [])),
                json.dumps(memory.metadata)
            ))
            
            # å­˜å‚¨å‘é‡
            if CHROMADB_AVAILABLE and memory.embedding:
                self.collection.upsert(
                    ids=[memory.id],
                    embeddings=[memory.embedding],
                    metadatas=[{
                        "memory_type": memory.memory_type,
                        "importance": memory.importance,
                        "timestamp": memory.timestamp,
                        "tier": "medium"  # é»˜è®¤ä¸­æœŸï¼Œç¨åä¼šæ›´æ–°
                    }],
                    documents=[memory.content]
                )
            else:
                # ç®€å•å‘é‡å­˜å‚¨
                self.vector_storage["ids"].append(memory.id)
                self.vector_storage["embeddings"].append(memory.embedding)
                self._save_vector_storage()
            # åˆ†é…è®°å¿†å±‚çº§
            try:
                assigned_tier = await self.tier_manager.assign_memory_tier(
                    memory.id, 
                    memory.memory_type, 
                    memory.metadata.get("structured_info", {}), 
                    memory.importance
                )
                
                # æ›´æ–°å‘é‡æ•°æ®åº“ä¸­çš„å±‚çº§ä¿¡æ¯
                if CHROMADB_AVAILABLE:
                    try:
                        self.collection.update(
                            ids=[memory.id],
                            metadatas=[{
                                "memory_type": memory.memory_type,
                                "importance": memory.importance,
                                "timestamp": memory.timestamp,
                                "tier": assigned_tier.value  # ä½¿ç”¨ assigned_tier
                            }]
                        )
                    except Exception as e:
                        logger.debug(f"æ›´æ–°å‘é‡å±‚çº§ä¿¡æ¯å¤±è´¥: {e}")
                        
            except Exception as e:
                logger.debug(f"åˆ†é…è®°å¿†å±‚çº§å¤±è´¥: {e}")
            # ç¼“å­˜
            self.memory_cache[memory.id] = memory
            # æ·»åŠ åˆ°è®°å¿†å›¾è°±
            try:
                context_info = memory.metadata.get("context", "")  # ä»metadataè·å–context
                asyncio.create_task(self.graph_manager.add_memory_advanced(
                    memory.id, memory.content, memory.memory_type, memory.importance,
                    context_info
                ))
            except Exception as e:
                logger.debug(f"æ·»åŠ åˆ°å¢å¼ºå›¾è°±å¤±è´¥: {e}")
            
            self.conn.commit()
            
        except Exception as e:
            logger.error(f"å­˜å‚¨è®°å¿†å¤±è´¥: {e}")
    
    async def smart_search_and_respond(self, question: str, context: str = "") -> Tuple[str, List[str]]:
        """æ™ºèƒ½æœç´¢å¹¶ç”Ÿæˆå›ç­”ï¼Œè¿”å›(å›ç­”, ä½¿ç”¨çš„è®°å¿†IDåˆ—è¡¨) - å›¾è°±å¢å¼ºç‰ˆ"""
        
        """æ™ºèƒ½æœç´¢å¹¶ç”Ÿæˆå›ç­” - å¢å¼ºèº«ä»½è§£æ"""
        try:
            logger.info(f"ğŸ” æ™ºèƒ½æœç´¢é—®é¢˜: {question}")
            
            # 1. å°è¯•èº«ä»½å¢å¼ºæœç´¢
            try:
                return await self.identity_resolver.enhance_search_with_identity(question, self)
            except Exception as e:
                logger.debug(f"èº«ä»½å¢å¼ºæœç´¢å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæœç´¢: {e}")
            
            # 1. å‘é‡æœç´¢ç›¸å…³è®°å¿†
            relevant_memories = await self._vector_search(question)
            
            if not relevant_memories:
                logger.info("ğŸ“­ æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
                return "", []
            
            # æ”¶é›†ä½¿ç”¨çš„è®°å¿†ID
            used_memory_ids = [memory.id for memory in relevant_memories]
            
            # 2. å°è¯•å¢å¼ºå›¾è°±æœç´¢
            try:
                graph_response, graph_memory_ids = await self.graph_manager.enhanced_search_and_respond(question, context)
                
                if graph_response and "æŠ±æ­‰" not in graph_response:
                    # å›¾è°±æˆåŠŸæ‰¾åˆ°å…³è”ç­”æ¡ˆ
                    logger.info(f"âœ… å¢å¼ºå›¾è°±å›ç­”: {graph_response}")
                    used_memory_ids.extend(graph_memory_ids)
                    
                    # è®°å½•è®¿é—®
                    for memory_id in set(used_memory_ids):
                        asyncio.create_task(self.importance_adjuster.track_memory_access(
                            memory_id, context_relevant=True
                        ))
                    
                    return graph_response, list(set(used_memory_ids))

            except Exception as e:
                logger.debug(f"å¢å¼ºå›¾è°±æœç´¢å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæœç´¢: {e}")
            
            # 3. æ™®é€šæ™ºèƒ½å›ç­”ç”Ÿæˆï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
            response = await self._generate_smart_response(question, relevant_memories, context)
            
            # è®°å½•è®¿é—®
            for memory in relevant_memories:
                asyncio.create_task(self.importance_adjuster.track_memory_access(
                    memory.id, context_relevant=True
                ))
            
            logger.info(f"âœ… æ™®é€šæ™ºèƒ½å›ç­”: {response}")
            return response, used_memory_ids
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æœç´¢å›ç­”å¤±è´¥: {e}")
            return "", []
    
    async def _vector_search(self, query: str, top_k: int = 5) -> List[MemoryItem]:
        """å‘é‡ç›¸ä¼¼åº¦æœç´¢"""
        try:
            query_embedding = await self._get_embedding(query)
            if not query_embedding:
                return []
            
            if CHROMADB_AVAILABLE:
                try:
                    # ç®€åŒ–åˆ†å±‚æœç´¢ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°ç»“æœ
                    all_results = []
                    
                    # 1. å…ˆå°è¯•åˆ†å±‚æœç´¢
                    for tier in ["core", "medium", "short", "temporary"]:
                        try:
                            tier_results = self.collection.query(
                                query_embeddings=[query_embedding],
                                n_results=top_k//2,
                                where={"tier": tier}
                            )
                            if tier_results["ids"] and tier_results["ids"][0]:
                                all_results.extend(tier_results["ids"][0])
                                if len(all_results) >= top_k//2:  # æ‰¾åˆ°è¶³å¤Ÿç»“æœå°±åœæ­¢
                                    break
                        except Exception as e:
                            logger.debug(f"æœç´¢ {tier} å±‚å¤±è´¥: {e}")
                            continue
                    
                    # 2. å¦‚æœåˆ†å±‚æœç´¢ç»“æœä¸è¶³ï¼Œä½¿ç”¨æ™®é€šæœç´¢è¡¥å……
                    if len(all_results) < top_k//2:
                        logger.debug("åˆ†å±‚æœç´¢ç»“æœä¸è¶³ï¼Œè¡¥å……æ™®é€šæœç´¢")
                        try:
                            fallback_results = self.collection.query(
                                query_embeddings=[query_embedding],
                                n_results=top_k
                            )
                            if fallback_results["ids"] and fallback_results["ids"][0]:
                                # åˆå¹¶ç»“æœï¼Œå»é‡
                                new_ids = [id for id in fallback_results["ids"][0] if id not in all_results]
                                all_results.extend(new_ids)
                        except Exception as e:
                            logger.debug(f"æ™®é€šæœç´¢ä¹Ÿå¤±è´¥: {e}")
                    
                    memory_ids = all_results[:top_k]
                    
                except Exception as e:
                    logger.debug(f"åˆ†å±‚æœç´¢å®Œå…¨å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æœç´¢: {e}")
                    # æœ€ç»ˆå›é€€
                    results = self.collection.query(
                        query_embeddings=[query_embedding],
                        n_results=top_k
                    )
                    memory_ids = results["ids"][0] if results["ids"] and results["ids"][0] else []
            else:
                # ç®€å•å‘é‡æœç´¢ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
                memory_ids = self._simple_vector_search(query_embedding, top_k)
            
            # ä»æ•°æ®åº“è·å–å®Œæ•´è®°å¿†
            memories = []
            for memory_id in memory_ids:
                memory = await self._get_memory_by_id(memory_id)
                if memory:
                    memories.append(memory)
                    # è®°å½•è®¿é—®ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰
                    asyncio.create_task(self.importance_adjuster.track_memory_access(
                        memory_id, context_relevant=True
                    ))
            
            logger.debug(f"ğŸ” å‘é‡æœç´¢æ‰¾åˆ° {len(memories)} æ¡ç›¸å…³è®°å¿†")
            return memories
            
        except Exception as e:
            logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
        
    async def get_tier_analytics(self) -> Dict[str, Any]:
        """è·å–åˆ†å±‚å­˜å‚¨åˆ†æ"""
        try:
            # åŸºç¡€åˆ†å±‚ç»Ÿè®¡
            tier_distribution = await self.tier_manager.get_tier_distribution()
            
            # å±‚çº§æ€§èƒ½åˆ†æ
            tier_performance = await self._analyze_tier_performance()
            
            return {
                "tier_distribution": tier_distribution,
                "tier_performance": tier_performance,
                "analysis_time": time.time()
            }
            
        except Exception as e:
            logger.error(f"è·å–åˆ†å±‚åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}

    async def _analyze_tier_performance(self) -> Dict[str, Any]:
        """åˆ†æå„å±‚çº§æ€§èƒ½"""
        try:
            performance = {}
            
            for tier in ["core", "medium", "short", "temporary"]:
                cursor = self.conn.execute("""
                    SELECT 
                        COUNT(DISTINCT m.id) as memory_count,
                        AVG(m.importance) as avg_importance,
                        AVG(u.access_count) as avg_access,
                        COUNT(CASE WHEN u.access_count > 5 THEN 1 END) as high_access_count
                    FROM memories m
                    LEFT JOIN memory_usage u ON m.id = u.memory_id
                    WHERE m.tier = ?
                """, (tier,))
                
                result = cursor.fetchone()
                if result:
                    performance[tier] = {
                        "memory_count": result[0],
                        "avg_importance": round(result[1] or 0, 3),
                        "avg_access": round(result[2] or 0, 2),
                        "high_access_ratio": round((result[3] or 0) / max(result[0], 1), 3)
                    }
            
            return performance
            
        except Exception as e:
            logger.error(f"åˆ†æå±‚çº§æ€§èƒ½å¤±è´¥: {e}")
            return {}
    
    def _simple_vector_search(self, query_embedding: List[float], top_k: int) -> List[str]:
        """ç®€å•å‘é‡æœç´¢"""
        try:
            if not self.vector_storage["embeddings"]:
                return []
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            query_vec = np.array(query_embedding)
            similarities = []
            
            for i, embedding in enumerate(self.vector_storage["embeddings"]):
                if embedding:
                    emb_vec = np.array(embedding)
                    similarity = np.dot(query_vec, emb_vec) / (
                        np.linalg.norm(query_vec) * np.linalg.norm(emb_vec)
                    )
                    similarities.append((similarity, self.vector_storage["ids"][i]))
            
            # æ’åºå¹¶è¿”å›top_k
            similarities.sort(key=lambda x: x[0], reverse=True)
            
            return [item[1] for item in similarities[:top_k] 
                   if item[0] >= self.similarity_threshold]
            
        except Exception as e:
            logger.error(f"ç®€å•å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _get_memory_by_id(self, memory_id: str) -> Optional[MemoryItem]:
        """æ ¹æ®IDè·å–è®°å¿†"""
        try:
            # å…ˆæ£€æŸ¥ç¼“å­˜
            if memory_id in self.memory_cache:
                return self.memory_cache[memory_id]
            
            # ä»æ•°æ®åº“æŸ¥è¯¢
            cursor = self.conn.execute("""
                SELECT id, content, memory_type, importance, timestamp, metadata
                FROM memories WHERE id = ?
            """, (memory_id,))
            
            row = cursor.fetchone()
            if row:
                memory = MemoryItem(
                    id=row[0],
                    content=row[1],
                    memory_type=row[2],
                    importance=row[3],
                    timestamp=row[4],
                    metadata=json.loads(row[5]) if row[5] else {}
                )
                
                # ç¼“å­˜
                self.memory_cache[memory_id] = memory
                return memory
            
            return None
            
        except Exception as e:
            logger.error(f"è·å–è®°å¿†å¤±è´¥: {e}")
            return None
    
    async def _generate_smart_response(self, question: str, memories: List[MemoryItem], 
                                     context: str = "") -> str:
        """ç”Ÿæˆæ™ºèƒ½å›ç­”"""
        try:
            # æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡
            memory_context = self._build_memory_context(memories)
            
            prompt = f"""åŸºäºä»¥ä¸‹ä¸ªäººè®°å¿†ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ç”Ÿæˆè‡ªç„¶ã€ä¸ªæ€§åŒ–çš„å›ç­”ã€‚

é—®é¢˜: {question}

ä¸ªäººè®°å¿†:
{memory_context}

å¯¹è¯ä¸Šä¸‹æ–‡: {context}

è¦æ±‚:
1. å›ç­”è¦è‡ªç„¶ã€å£è¯­åŒ–
2. åŸºäºè®°å¿†ä¸­çš„çœŸå®ä¿¡æ¯å›ç­”
3. å¦‚æœè®°å¿†ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨
4. å¦‚æœæ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç®€å•è¯´æ˜æ²¡æœ‰ç›¸å…³è®°å½•
5. 1-2å¥è¯ç®€æ´å›ç­”
6. ç”¨ç¬¬ä¸€äººç§°å›ç­”ï¼ˆæˆ‘ã€æˆ‘çš„ï¼‰

è¯·ç›´æ¥ç»™å‡ºå›ç­”ï¼š"""

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=150,
                temperature=0.7
            )
            
            if response.choices:
                answer = response.choices[0].message.content.strip()
                return answer
            
            return ""
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ™ºèƒ½å›ç­”å¤±è´¥: {e}")
            return ""
    
    def _build_memory_context(self, memories: List[MemoryItem]) -> str:
        """æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡"""
        context_lines = []
        
        # æŒ‰é‡è¦æ€§å’Œæ—¶é—´æ’åº
        sorted_memories = sorted(memories, 
                               key=lambda x: (x.importance, x.timestamp), 
                               reverse=True)
        
        for memory in sorted_memories[:10]:  # æœ€å¤š10æ¡è®°å¿†
            summary = memory.metadata.get("summary", memory.content)
            structured = memory.metadata.get("structured_info", {})
            
            # ä¼˜å…ˆæ˜¾ç¤ºç»“æ„åŒ–ä¿¡æ¯
            if structured:
                info_parts = []
                for key, value in structured.items():
                    if value:
                        info_parts.append(f"{key}: {value}")
                
                if info_parts:
                    context_lines.append(f"- {', '.join(info_parts)} (é‡è¦æ€§: {memory.importance:.2f})")
                    continue
            
            # å¦åˆ™æ˜¾ç¤ºæ‘˜è¦
            context_lines.append(f"- {summary} (é‡è¦æ€§: {memory.importance:.2f})")
        
        return '\n'.join(context_lines) if context_lines else "æš‚æ— ç›¸å…³è®°å¿†"
    
    async def _search_by_type(self, memory_type: str) -> List[MemoryItem]:
        """æŒ‰ç±»å‹æœç´¢è®°å¿†"""
        try:
            cursor = self.conn.execute("""
                SELECT id FROM memories WHERE memory_type = ? 
                ORDER BY importance DESC, timestamp DESC
            """, (memory_type,))
            
            memory_ids = [row[0] for row in cursor.fetchall()]
            
            memories = []
            for memory_id in memory_ids:
                memory = await self._get_memory_by_id(memory_id)
                if memory:
                    memories.append(memory)
            
            return memories
            
        except Exception as e:
            logger.error(f"æŒ‰ç±»å‹æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _mark_memory_outdated(self, memory_id: str):
        """æ ‡è®°è®°å¿†ä¸ºè¿‡æœŸ"""
        try:
            self.conn.execute("""
                UPDATE memories SET importance = importance * 0.1
                WHERE id = ?
            """, (memory_id,))
            
            self.conn.commit()
            
            # æ¸…é™¤ç¼“å­˜
            if memory_id in self.memory_cache:
                del self.memory_cache[memory_id]
            
        except Exception as e:
            logger.error(f"æ ‡è®°è®°å¿†è¿‡æœŸå¤±è´¥: {e}")
    
    def _save_vector_storage(self):
        """ä¿å­˜å‘é‡å­˜å‚¨"""
        try:
            with open(self.vector_file, 'wb') as f:
                pickle.dump(self.vector_storage, f)
        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡å­˜å‚¨å¤±è´¥: {e}")
    
    async def cleanup_old_memories(self, days: int = 30):
        """æ¸…ç†æ—§è®°å¿†"""
        current_time = time.time()
        try:
            cutoff_time = time.time() - (days * 24 * 3600)
            
            cursor = self.conn.execute("""
                DELETE FROM memories 
                WHERE timestamp < ? AND importance < ?
            """, (cutoff_time, 0.5))
            
            deleted_count = cursor.rowcount
            self.conn.commit()
            
            logger.info(f"ğŸ§¹ æ¸…ç†äº† {deleted_count} æ¡æ—§è®°å¿†")
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§è®°å¿†å¤±è´¥: {e}")
        if current_time % 60 < 10:  # 6å°æ—¶ = 21600ç§’
            try:
                if hasattr(self.openai_memory_manager, 'importance_adjuster'):
                    adjustments = await self.openai_memory_manager.importance_adjuster.adjust_memory_importance_by_usage()
                    if adjustments:
                        logger.info(f"ğŸ“Š é‡è¦æ€§è°ƒæ•´: {len(adjustments)} æ¡è®°å¿†")
            except Exception as e:
                logger.debug(f"é‡è¦æ€§è°ƒæ•´å¤±è´¥: {e}")
        if current_time % 60 < 10:  # 12å°æ—¶ = 43200ç§’
            try:
                if hasattr(self.openai_memory_manager, 'tier_manager'):
                    transitions = await self.openai_memory_manager.tier_manager.review_and_adjust_tiers()
                    if transitions:
                        logger.info(f"ğŸ“‚ å±‚çº§è°ƒæ•´: {len(transitions)} æ¡è®°å¿†è½¬æ¢")
            except Exception as e:
                logger.debug(f"å±‚çº§å®¡æŸ¥å¤±è´¥: {e}")

    async def get_memory_analytics(self) -> Dict[str, Any]:
        """è·å–è®°å¿†åˆ†ææ•°æ®"""
        try:
            # åŸºç¡€ç»Ÿè®¡
            basic_stats = await self.get_memory_stats()
            
            # é‡è¦æ€§è¶‹åŠ¿
            importance_trends = await self.importance_adjuster.get_importance_trends()
            
            # çƒ­é—¨è®°å¿†
            top_memories = await self.importance_adjuster.get_top_accessed_memories()
            
            return {
                "basic_stats": basic_stats,
                "importance_trends": importance_trends,
                "top_memories": top_memories,
                "analysis_time": time.time()
            }
            
        except Exception as e:
            logger.error(f"è·å–è®°å¿†åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç»Ÿè®¡"""
        try:
            cursor = self.conn.execute("""
                SELECT 
                    COUNT(*) as total_memories,
                    AVG(importance) as avg_importance,
                    memory_type,
                    COUNT(*) as type_count
                FROM memories 
                GROUP BY memory_type
            """)
            
            stats = {"total_memories": 0, "by_type": {}}
            
            for row in cursor.fetchall():
                if row[2]:  # memory_type
                    stats["by_type"][row[2]] = row[3]
                    if stats["total_memories"] == 0:
                        stats["total_memories"] = row[0]
                        stats["avg_importance"] = row[1]
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–è®°å¿†ç»Ÿè®¡å¤±è´¥: {e}")
            return {"total_memories": 0, "by_type": {}}
    
    def close(self):
        """å…³é—­è¿æ¥"""
        try:
            if hasattr(self, 'conn'):
                self.conn.close()
            
            # ä¿å­˜å‘é‡å­˜å‚¨
            if not CHROMADB_AVAILABLE:
                self._save_vector_storage()
            
            logger.info("âœ… OpenAIè®°å¿†ç®¡ç†å™¨å·²å…³é—­")
            
        except Exception as e:
            logger.error(f"å…³é—­è®°å¿†ç®¡ç†å™¨å¤±è´¥: {e}")