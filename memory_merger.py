"""
æ™ºèƒ½è®°å¿†å»é‡å’Œåˆå¹¶æ¨¡å—
è´Ÿè´£æ£€æµ‹ç›¸ä¼¼è®°å¿†å¹¶æ™ºèƒ½åˆå¹¶
"""
import json
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from loguru import logger

@dataclass
class MergeResult:
    """åˆå¹¶ç»“æœ"""
    should_merge: bool
    merge_strategy: str  # "replace", "combine", "keep_both"
    merged_content: str
    merged_structured_info: Dict[str, Any]
    confidence: float
    reason: str

class MemoryMerger:
    """æ™ºèƒ½è®°å¿†åˆå¹¶å™¨"""
    
    def __init__(self, openai_client):
        self.openai_client = openai_client
        self.similarity_threshold = 0.8
        self.merge_confidence_threshold = 0.7
        
        logger.info("ğŸ”„ æ™ºèƒ½è®°å¿†åˆå¹¶å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def check_and_merge_similar_memories(self, new_memory, existing_memories: List) -> Optional[MergeResult]:
        """æ£€æŸ¥å¹¶åˆå¹¶ç›¸ä¼¼è®°å¿†"""
        try:
            # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„è®°å¿†
            most_similar = await self._find_most_similar_memory(new_memory, existing_memories)
            
            if not most_similar:
                return None
            
            similar_memory, similarity_score = most_similar
            
            if similarity_score < self.similarity_threshold:
                return None
            
            logger.info(f"ğŸ” å‘ç°ç›¸ä¼¼è®°å¿†ï¼Œç›¸ä¼¼åº¦: {similarity_score:.2f}")
            logger.info(f"   æ–°è®°å¿†: {new_memory.content}")
            logger.info(f"   ç›¸ä¼¼è®°å¿†: {similar_memory.content}")
            
            # ä½¿ç”¨GPTåˆ¤æ–­åˆå¹¶ç­–ç•¥
            merge_result = await self._gpt_merge_analysis(new_memory, similar_memory)
            
            if merge_result.should_merge and merge_result.confidence >= self.merge_confidence_threshold:
                logger.info(f"âœ… è®°å¿†åˆå¹¶: {merge_result.merge_strategy} (ç½®ä¿¡åº¦: {merge_result.confidence:.2f})")
                logger.info(f"   åˆå¹¶ç†ç”±: {merge_result.reason}")
                return merge_result
            
            return None
            
        except Exception as e:
            logger.error(f"è®°å¿†åˆå¹¶æ£€æŸ¥å¤±è´¥: {e}")
            return None
    
    async def _find_most_similar_memory(self, new_memory, existing_memories: List) -> Optional[Tuple]:
        """æ‰¾åˆ°æœ€ç›¸ä¼¼çš„è®°å¿†"""
        try:
            if not existing_memories:
                return None
            
            max_similarity = 0.0
            most_similar_memory = None
            
            new_content = new_memory.content.lower()
            new_type = new_memory.memory_type
            new_structured = new_memory.metadata.get("structured_info", {})
            
            for existing_memory in existing_memories:
                # åªæ¯”è¾ƒç›¸åŒç±»å‹çš„è®°å¿†
                if existing_memory.memory_type != new_type:
                    continue
                
                existing_content = existing_memory.content.lower()
                existing_structured = existing_memory.metadata.get("structured_info", {})
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self._calculate_similarity(
                    new_content, existing_content,
                    new_structured, existing_structured
                )
                
                if similarity > max_similarity:
                    max_similarity = similarity
                    most_similar_memory = existing_memory
            
            if most_similar_memory and max_similarity > 0.6:
                return (most_similar_memory, max_similarity)
            
            return None
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾ç›¸ä¼¼è®°å¿†å¤±è´¥: {e}")
            return None
    
    def _calculate_similarity(self, content1: str, content2: str, 
                            structured1: Dict, structured2: Dict) -> float:
        """è®¡ç®—è®°å¿†ç›¸ä¼¼åº¦"""
        try:
            # æ–‡æœ¬ç›¸ä¼¼åº¦ (ç®€å•çš„è¯æ±‡é‡å )
            words1 = set(content1.split())
            words2 = set(content2.split())
            
            if not words1 or not words2:
                text_similarity = 0.0
            else:
                intersection = len(words1.intersection(words2))
                union = len(words1.union(words2))
                text_similarity = intersection / union if union > 0 else 0.0
            
            # ç»“æ„åŒ–ä¿¡æ¯ç›¸ä¼¼åº¦
            struct_similarity = self._calculate_structured_similarity(structured1, structured2)
            
            # ç»¼åˆç›¸ä¼¼åº¦ (æ–‡æœ¬60% + ç»“æ„åŒ–40%)
            total_similarity = text_similarity * 0.6 + struct_similarity * 0.4
            
            return total_similarity
            
        except Exception as e:
            logger.debug(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_structured_similarity(self, struct1: Dict, struct2: Dict) -> float:
        """è®¡ç®—ç»“æ„åŒ–ä¿¡æ¯ç›¸ä¼¼åº¦"""
        try:
            if not struct1 or not struct2:
                return 0.0
            
            # æ£€æŸ¥å…³é”®å­—æ®µçš„é‡å 
            key_fields = ["name", "job", "location", "height", "weight", "age"]
            
            matches = 0
            total_fields = 0
            
            for field in key_fields:
                val1 = struct1.get(field)
                val2 = struct2.get(field)
                
                if val1 or val2:
                    total_fields += 1
                    if val1 and val2 and str(val1).lower() == str(val2).lower():
                        matches += 1
            
            return matches / total_fields if total_fields > 0 else 0.0
            
        except Exception as e:
            logger.debug(f"è®¡ç®—ç»“æ„åŒ–ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0
    
    async def _gpt_merge_analysis(self, new_memory, existing_memory) -> MergeResult:
        """ä½¿ç”¨GPTåˆ†æåˆå¹¶ç­–ç•¥"""
        try:
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è®°å¿†ç®¡ç†ä¸“å®¶ï¼Œéœ€è¦åˆ¤æ–­ä¸¤æ¡ç›¸ä¼¼çš„è®°å¿†æ˜¯å¦åº”è¯¥åˆå¹¶ï¼Œä»¥åŠå¦‚ä½•åˆå¹¶ã€‚

æ–°è®°å¿†:
- å†…å®¹: {new_memory.content}
- ç±»å‹: {new_memory.memory_type}
- é‡è¦æ€§: {new_memory.importance}
- ç»“æ„åŒ–ä¿¡æ¯: {json.dumps(new_memory.metadata.get('structured_info', {}), ensure_ascii=False)}

å·²å­˜åœ¨è®°å¿†:
- å†…å®¹: {existing_memory.content}
- ç±»å‹: {existing_memory.memory_type}
- é‡è¦æ€§: {existing_memory.importance}
- ç»“æ„åŒ–ä¿¡æ¯: {json.dumps(existing_memory.metadata.get('structured_info', {}), ensure_ascii=False)}

è¯·åˆ†æå¹¶è¾“å‡ºJSONæ ¼å¼ç»“æœï¼š

{{
    "should_merge": true/false,
    "merge_strategy": "replace/combine/keep_both",
    "merged_content": "åˆå¹¶åçš„å†…å®¹",
    "merged_structured_info": {{"åˆå¹¶åçš„ç»“æ„åŒ–ä¿¡æ¯"}},
    "confidence": 0.0-1.0,
    "reason": "åˆå¹¶ç†ç”±"
}}

åˆå¹¶ç­–ç•¥è¯´æ˜ï¼š
- replace: æ–°ä¿¡æ¯æ›´å‡†ç¡®/è¯¦ç»†ï¼Œæ›¿æ¢æ—§ä¿¡æ¯
- combine: ä¸¤æ¡ä¿¡æ¯äº’è¡¥ï¼Œåˆå¹¶ä¸ºæ›´å®Œæ•´çš„ä¿¡æ¯
- keep_both: ä¿¡æ¯ä¸åŒä½†éƒ½é‡è¦ï¼Œä¿ç•™ä¸¤æ¡

åˆ¤æ–­æ ‡å‡†ï¼š
1. å¦‚æœæ˜¯åŒä¸€ä¿¡æ¯çš„ä¸åŒè¡¨è¿° â†’ should_merge: true, strategy: replace/combine
2. å¦‚æœæ–°ä¿¡æ¯æ›´è¯¦ç»†å‡†ç¡® â†’ should_merge: true, strategy: replace
3. å¦‚æœä¿¡æ¯äº’è¡¥ â†’ should_merge: true, strategy: combine
4. å¦‚æœä¿¡æ¯çŸ›ç›¾ä¸”æ— æ³•åˆ¤æ–­ â†’ should_merge: false, strategy: keep_both

ç¤ºä¾‹ï¼š
- "æˆ‘ä½åœ¨åŒ—äº¬" + "æˆ‘å®¶åœ¨åŒ—äº¬æœé˜³åŒº" â†’ combine: "æˆ‘ä½åœ¨åŒ—äº¬æœé˜³åŒº"
- "æˆ‘èº«é«˜180cm" + "æˆ‘æœ‰ä¸€ç±³å…«" â†’ replace: "æˆ‘èº«é«˜180cm"
- "æˆ‘å¦ˆå¦ˆæ˜¯åŒ»ç”Ÿ" + "æˆ‘å¦ˆå¦ˆæ˜¯è€å¸ˆ" â†’ keep_both (éœ€è¦ç”¨æˆ·ç¡®è®¤)"""

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
                temperature=0.1
            )
            
            if response.choices:
                result_text = response.choices[0].message.content.strip()
                try:
                    result_data = json.loads(result_text)
                    
                    return MergeResult(
                        should_merge=result_data.get("should_merge", False),
                        merge_strategy=result_data.get("merge_strategy", "keep_both"),
                        merged_content=result_data.get("merged_content", new_memory.content),
                        merged_structured_info=result_data.get("merged_structured_info", {}),
                        confidence=result_data.get("confidence", 0.0),
                        reason=result_data.get("reason", "GPTåˆ†æç»“æœ")
                    )
                    
                except json.JSONDecodeError:
                    logger.warning("GPTåˆå¹¶åˆ†æJSONè§£æå¤±è´¥")
            
            # å¤‡ç”¨ç­–ç•¥
            return self._fallback_merge_analysis(new_memory, existing_memory)
            
        except Exception as e:
            logger.error(f"GPTåˆå¹¶åˆ†æå¤±è´¥: {e}")
            return self._fallback_merge_analysis(new_memory, existing_memory)
    
    def _fallback_merge_analysis(self, new_memory, existing_memory) -> MergeResult:
        """å¤‡ç”¨åˆå¹¶åˆ†æ"""
        # ç®€å•è§„åˆ™ï¼šå¦‚æœæ–°è®°å¿†æ›´é•¿æ›´è¯¦ç»†ï¼Œæ›¿æ¢æ—§è®°å¿†
        if len(new_memory.content) > len(existing_memory.content) * 1.2:
            return MergeResult(
                should_merge=True,
                merge_strategy="replace",
                merged_content=new_memory.content,
                merged_structured_info=new_memory.metadata.get("structured_info", {}),
                confidence=0.8,
                reason="æ–°è®°å¿†æ›´è¯¦ç»†"
            )
        
        # å¦åˆ™ä¿ç•™ä¸¤æ¡
        return MergeResult(
            should_merge=False,
            merge_strategy="keep_both",
            merged_content="",
            merged_structured_info={},
            confidence=0.5,
            reason="æ— æ³•ç¡®å®šåˆå¹¶ç­–ç•¥"
        )
    
    async def merge_memories(self, target_memory, merge_result: MergeResult) -> Dict[str, Any]:
        """æ‰§è¡Œè®°å¿†åˆå¹¶"""
        try:
            if merge_result.merge_strategy == "replace":
                # æ›¿æ¢ç­–ç•¥ï¼šä½¿ç”¨æ–°å†…å®¹ï¼Œä¿æŒæ›´é«˜çš„é‡è¦æ€§
                merged_memory = {
                    "content": merge_result.merged_content,
                    "importance": max(target_memory.importance, target_memory.importance * 1.1),
                    "metadata": {
                        **target_memory.metadata,
                        "structured_info": merge_result.merged_structured_info,
                        "merge_history": {
                            "strategy": "replace",
                            "timestamp": time.time(),
                            "reason": merge_result.reason,
                            "original_content": target_memory.content
                        }
                    }
                }
                
            elif merge_result.merge_strategy == "combine":
                # åˆå¹¶ç­–ç•¥ï¼šç»„åˆä¿¡æ¯
                merged_memory = {
                    "content": merge_result.merged_content,
                    "importance": max(target_memory.importance, target_memory.importance * 1.2),
                    "metadata": {
                        **target_memory.metadata,
                        "structured_info": {
                            **target_memory.metadata.get("structured_info", {}),
                            **merge_result.merged_structured_info
                        },
                        "merge_history": {
                            "strategy": "combine",
                            "timestamp": time.time(),
                            "reason": merge_result.reason,
                            "combined_from": [target_memory.content]
                        }
                    }
                }
            
            else:  # keep_both
                return None
            
            logger.info(f"âœ… è®°å¿†åˆå¹¶å®Œæˆ: {merge_result.merge_strategy}")
            return merged_memory
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œè®°å¿†åˆå¹¶å¤±è´¥: {e}")
            return None